import os
# Disable ChromaDB anonymized telemetry
os.environ['ANONYMIZED_TELEMETRY'] = 'False'

import logging
import pandas as pd
from pathlib import Path
from typing import Iterable
from chatwhatcartobuy.utils.getpath import get_latest_path, get_repo_root
from chatwhatcartobuy.rag.embeddings import build_embedding_model
from chatwhatcartobuy.config.logging_config import setup_logging
from chatwhatcartobuy.rag.documents import get_documents_and_ids, preprocess_raw_parquet
from langchain_chroma.vectorstores import Chroma

logger = logging.getLogger(__name__)

def parse_latest_data(search_pat: str):
    """
    Parse the most recent parquet files generated by the prepare_data.py script (Merging raw
    parquet files, wrangling, and exporting to data/processed/partition directory).
    
    Args:
        search_pat: PosixPath search pattern.
        
    Returns:
        dataframe: Pandas DataFrame for submissions or comments.
    """
    file_path = get_latest_path(search_pat)
    logger.debug('Parsing parquet file from path: %s', file_path)
    return pd.read_parquet(path=file_path, engine='pyarrow')    

def build_vector_db(embeddings, collection_name: str, persist_directory: str | Path, **kwargs):
    """
    Builds a ChromaDB vector database persisted to disk.
    
    Args: 
        embeddings: Embedding model or function as arg for Chroma object instantiation.
        collection_name: ChromaDB collection name.
        persist_directory: Directory path where vector db will be stored.
        
    Returns:
        Chroma: In-memory vector store persisted in a subdirectory in project root.
    """
    logging.info('Building ChromaDB vector store for collection: %s', collection_name)
    
    if isinstance(persist_directory, Path):
            persist_directory = str(persist_directory)
    
    logging.debug('Creating a ChromaDB client with args: collection_name=%s, embedding_function=%r, persist_directory=%s', 
                 collection_name, embeddings, persist_directory)
    
    vector_db = Chroma(
        **kwargs,
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=persist_directory
    )
    
    return vector_db

def load_vector_db(embeddings, collection_name: str, persist_directory: str | Path, **kwargs):
    """
    Loads the vector database from disk. Ensure collection_name and persist_directory exactly match.
    
    Args: 
        embeddings: Embedding model or function as arg for Chroma object instantiation.
        collection_name: ChromaDB collection name.
        persist_directory: Directory path where vector db will be stored.
        
    Returns:
        Chroma: In-memory vector store persisted in a subdirectory in project root.
    """
    logging.info('Loading Chroma database collection "%s" from path: %s', collection_name, persist_directory)
    
    if isinstance(persist_directory, Path):
        persist_directory = str(persist_directory)
    
    return Chroma(
        **kwargs,
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=persist_directory
    )

def split_to_batches(iterable: Iterable, batch_size:int=5000):
    """
    Yield successive batches from an iterable (either documents or ids).
    
    Args:
        iterable: 
    """
    for i in range(0, len(iterable), batch_size):
        logger.debug('Yielding batch for records: %d-%d', i, i+batch_size-1)
        yield iterable[i: i+batch_size]

if __name__ == '__main__':
    # When this script is run, create a Chroma vector store with documents from the latest
    # preprocessed parquet files then save to project_root/chromadb/.
    
    setup_logging(level=logging.INFO, output_to_console=True)
    
    # SBERT embedding model; normalized for cosine similarity
    embeddings = build_embedding_model(device='mps', batch_size=256, normalize_embeddings=True)
    
    db_dir = str(get_repo_root()/'chromadb')
    vector_db = build_vector_db(embeddings, collection_name='rag-vector-db', persist_directory=db_dir)
    
    # Optional Merge and wrangle latest scraped data then save to disk
    submissions = preprocess_raw_parquet(get_latest_path('raw/submission-dataset/*/'))
    comments = preprocess_raw_parquet(get_latest_path('raw/comment-dataset/*/'))
    
    # Get the docs and ids from latest processed parquet files
    submission_docs, submission_ids = get_documents_and_ids(df=submissions)
    comment_docs, comment_ids = get_documents_and_ids(df=comments)
    
    # Batch add documents from submissions and comments
    for docs, ids in zip((submission_docs, comment_docs), (submission_ids, comment_ids)):
        for batch_docs, batch_ids in zip(split_to_batches(docs), split_to_batches(ids)):
            vector_db.add_documents(batch_docs, ids=batch_ids)