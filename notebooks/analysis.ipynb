{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d1e5ad",
   "metadata": {},
   "source": [
    "# Used Car Analysis from Reddit Discussions\n",
    "\n",
    "__Goal:__ Determine ideal used car models for my personal profile by leveraging crowd knowlege.\n",
    "\n",
    "__Objectives:__\n",
    "1. Establish access to Reddit API either via PRAW or Requests\n",
    "2. Extract and parse relevant submissions and respective top-level comments\n",
    "3. Explore and analyze textual data\n",
    "4. Build a recommender that takes into account my personal profile and returns a comprehensive discussion on the ideal used vehicle model/s for me.\n",
    "\n",
    "__Motivation:__ As a first-time car buyer, it's easy to get lost in the sea of options in the used car market. The plethora of options for brands, models, model years and trims, and whatnot, can make it daunting to do your own research. Personally, I find myself gravitating to online forums when doing my research on used cars as I believe in the concept of \"Wisdom of the crowd\", wherein majority sentiment may be indicative of some measurable and verifiable phenomena. \n",
    "\n",
    "In the case of used cars, if a significant number of people praise a specific Japanese-manufactured vehicle model while citing its faults, an algorithm that sufficiently captures this data may be able to present a high confidence level for the said model while presenting an overview of common issues to be aware of. In doing so, the work involved in research is cut in half, and prospective buyers can trim their options down to a few models that fit their criteria, while being made aware of the salient points that require extensive due diligence (e.g. common issues, upkeep, regulations) that only humans can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a7928",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044418d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas praw prawcore python-dotenv pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw, prawcore, time, os, sys, functools, random\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "from collections.abc import Callable, Iterator\n",
    "from itertools import product\n",
    "\n",
    "# Load .env file for access keys\n",
    "load_dotenv(os.path.join('..', 'config', '.env'))\n",
    "\n",
    "# Import config.py to access environment variables\n",
    "sys.path.append('../config')\n",
    "from config import PRAW_ID, PRAW_SECRET, PRAW_USER_AGENT, PRAW_USERNAME, PRAW_PASSWORD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bb44f",
   "metadata": {},
   "source": [
    "## Setting up access to Reddit API\n",
    "Access keys to Reddit API are stored in a .env file under the config directory of this repository. A template for the .env file is provided in the config directory.\n",
    "\n",
    "The config.py script assigns the environment variables to the `PRAW_ID`, `PRAW_SECRET`, `PRAW_USER_AGENT`, `PRAW_USERNAME`, and `PRAW_PASSWORD` global variables respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRAW \n",
    "reddit = praw.Reddit(\n",
    "    client_id = PRAW_ID,\n",
    "    client_secret = PRAW_SECRET,\n",
    "    username = PRAW_USERNAME,\n",
    "    password = PRAW_PASSWORD,\n",
    "    user_agent = PRAW_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01d930",
   "metadata": {},
   "source": [
    "## Extracting text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88863fd",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "The helper functions were designed to extract relevant data and metadata from Reddit submissions and comments, and package the data into a dict of dicts that can be easily parsed into a Pandas DataFrame object for further analysis.\n",
    "\n",
    "`backoff_on_rate_limit`: This is a decorator factory that builds a custom decorator based on specified backoff parameters (max retries, base delay, cap, jitter). The decorator itself is a wrapper for custom functions that call PRAW methods such as `fetch_submissions` and `fetch_comments`, which call subreddit.search() and submission.comments.replace_more() respectively. The decorator implements exponential backoff with optional full jitter to respect Reddit API rate limits while handling transient failures.\n",
    "\n",
    "__Inputs:__\n",
    "- Integer value for max retries. When attempts exceed this number, an Exception is raised\n",
    "- Float for base delay in seconds (i.e. Delay at first failed attempt)\n",
    "- Float for maximum delay in seconds\n",
    "- Bool on whether to implement full jitter or not\n",
    "\n",
    "__Outputs:__\n",
    "- Decorator to be applied to an PRAW API request wrapper\n",
    "\n",
    "`parse_comments`: This is a utility function that fetches comments from a given post and formats each comment as a dictionary of dictionaries with key as comment id and value as a dictionary of comment content and metadata (e.g. body, timestamp, upvotes).\n",
    "\n",
    "__Inputs:__ \n",
    "- Submission object from PRAW (i.e. Reddit posts)\n",
    "- Integer for .replace_more limit parameter, default=0 (i.e. top/parent comments only)\n",
    "\n",
    "__Output:__\n",
    "- Dict of comments in the format {comment_id : {data_header: data_value}}\n",
    "\n",
    "`parse_search_results`: This is a utility function that fetches submissions (posts) from a given subreddit using a predefined search query (i.e. keywords). Submissions are formatted into a dict of dicts with format {submission id : {data_header : data_value}}. This returns a tuple of submission data and comment data.\n",
    "\n",
    "__Inputs:__ \n",
    "- String of Subreddit name\n",
    "- String of search query\n",
    "- Integer for limit of submissions yielded by PRAW subreddit search\n",
    "\n",
    "__Output:__\n",
    "- Tuple of submission data dict and comment data dict\n",
    "\n",
    "__More reading materials:__\n",
    "1. [API Rate Limits Explained: Best Practices for 2025](https://orq.ai/blog/api-rate-limit)\n",
    "2. [Exponential Backoff And Jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "8a6d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_on_rate_limit(max_retries:int=5, \n",
    "                        base_delay:float=1.0, \n",
    "                        cap_delay:float=60.0, \n",
    "                        jitter:bool=True) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator factory that applies exponential backoff (with optional jitter)\n",
    "    when Reddit API rate limits (HTTP 429) or server errors occur.\n",
    "    Stops after max_retries and re-raises the exception.\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Start with base delay, then exponentially scale by attempt\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except prawcore.exceptions.ResponseException as e:\n",
    "                    if attempt > max_retries:\n",
    "                        raise Exception(\"Max retries exceeded with Reddit API.\")\n",
    "                    delay = min(cap_delay, base_delay * 2 ** attempt)\n",
    "                    if jitter:\n",
    "                        delay = random.uniform(0, delay)\n",
    "                    print(f\"[WARNING] {e.__class__.__name__} on attempt {attempt+1}, retrying after {delay:.2f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    attempt += 1\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_submissions(subreddit:object, query:str, limit:int=100, **kwargs) -> Iterator:\n",
    "    \"\"\"Modify the subreddit search from PRAw to ensure adherence to safe request limits.\"\"\"\n",
    "    return subreddit.search(**kwargs, query=query, limit=limit)\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_comments(submission:object, limit:int=0) -> list:\n",
    "    \"\"\"Modify the comment fetch from PRAW to ensure adherence to safe request limits.\"\"\"\n",
    "    submission.comments.replace_more(limit=limit)\n",
    "    return submission.comments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "acf4f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_comments(submission:object, limit:int=0):\n",
    "    \"\"\"Parses comments from a Reddit submission and packages it into a dict of dicts.\"\"\"\n",
    "    # Dict of dicts with format {comment_id : comment_info_dict}\n",
    "    comment_data: Dict[str, Dict[str, Any]] = {}\n",
    "    # Update comments dict with info dict \n",
    "    for comment in fetch_comments(submission, limit=limit):\n",
    "        comment_data[comment.id] = {\n",
    "            'body':comment.body,\n",
    "            'score':comment.score,\n",
    "            'timestamp':comment.created_utc,\n",
    "            'subreddit':comment.subreddit_name_prefixed,\n",
    "            'parent_submission_id':submission.id\n",
    "        }\n",
    "    return comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ff8547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_results(subreddit_name:str, query:str, limit:int=50, **search_kwargs):\n",
    "    \"\"\"\n",
    "    Fetch submissions/posts given a user-defined search query and returns a tuple of parsed submission\n",
    "    data and comment data. The parsed data is a dict of dicts where each key is a submission/comment id \n",
    "    and value is a dict of content and metadata for that particular submission/comment.\n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Dict of dicts with format {id : data_dict}\n",
    "    submission_data: Dict[str, Dict[str, Any]] = {}\n",
    "    comment_data: Dict[str, Dict[str, Any]] = {}\n",
    "    \n",
    "    # Fetch submissions, and for every submission, fetch the comments\n",
    "    for submission in fetch_submissions(**search_kwargs, subreddit=sub, query=query, limit=limit):\n",
    "        # Update submissions dict with info dict from submission\n",
    "        submission_data[submission.id] = {\n",
    "            'title':submission.title,\n",
    "            'selftext':submission.selftext,\n",
    "            'score':submission.score,\n",
    "            'upvote_ratio':submission.upvote_ratio,\n",
    "            'timestamp':submission.created_utc,\n",
    "            'subreddit':submission.subreddit_name_prefixed,\n",
    "            'num_comments':submission.num_comments\n",
    "            }\n",
    "        # Update comments dict with comments from the current submission\n",
    "        comment_data.update(parse_comments(submission))\n",
    "        \n",
    "    return (submission_data, comment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b0f1e",
   "metadata": {},
   "source": [
    "### Provide an initial list of search queries and Subreddits\n",
    "\n",
    "To scrape the relevant text data from Reddit, I created a small list of queries covering diverse yet relevant topics to buying affordable used vehicles. The queries involved location-specific, model-specific, and thematic keywords to ensure that the search covers as much ground as possible. Chosen subreddits have > 1e5 subscribers to ensure that search queries will yield a significant amount of results per API request.\n",
    "\n",
    "With a 10x10 query and subreddit array, I expect at least an initial 100 requests for the subreddit search yielding 100x50 submissions at most.\n",
    "\n",
    "Fetching the comments involves significantly more requests as each submission requires 1 request to yield the CommentForest. Fetching the comments will require at least 10,000 requests.\n",
    "\n",
    "__Expected Minimum API Requests__\n",
    "|Search Requests|Comment Fetch Requests|Total Requests|\n",
    "|:----------|:----------|:----------|\n",
    "|100      |5,000  |5,100|\n",
    "\n",
    "As such, a single batch job covering all query-subreddit combinations will yield at least 10,100 API requests in a single go, which wildly exceeds the Reddit API fair use policy (i.e. Cap requests to 100/min averaged over 10-minute sliding window). To address this issue, batched processing will be implemented to ensure average requests is under safe rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da66db6",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "`parse_txt_file`: Parses text files containing data separated by newlines. Returns a list. Used for containerizing search_queries and subreddit strings into separate text files that can be easily mutated without modifying source code.\n",
    "\n",
    "__Input:__\n",
    "- String for the path of text file, with each item separated by a newline\n",
    "\n",
    "__Output:__\n",
    "- List (e.g. search queries, subreddit names)\n",
    "\n",
    "`aggregate_search_results`: This function is a wrapper for the `parse_search_results` call and calls the inner function for each subreddit-query pair formed from the input list arguments. Requests are tracked at every iteration and compared against maximum requests per minute. If expected total requests go beyond rate limit, program execution is paused for at least a minute to ensure that requests are within safe rate limits. The number of submissions requested are also randomized per search pair to reduce predictability of scraping pattern.\n",
    "\n",
    "__Inputs:__ \n",
    "- List of subreddit name strings\n",
    "- List of search query strings\n",
    "- Int of maximum requests per minute, also determines upper bound of search result limit\n",
    "- Int of minimum requests, which is the floor of search result limit\n",
    "- List of float values denoting delay in seconds for long delay (interval between search pairs); minimum of 60s\n",
    "\n",
    "__Output:__\n",
    "- Tuple of aggregated submissions dict and comments dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "48b8744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_file(file_path:str):\n",
    "    \"\"\"\n",
    "    Utility function for parsing a multi-line text file where each item is separated\n",
    "    by a newline.\n",
    "    Input: String for file path\n",
    "    Output: List\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Ignore comments and empty lines\n",
    "        results = [line.rstrip(\"\\n\") for line in f if not (line.startswith('#') or line.startswith(\"\\n\"))]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9cf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_search_results(subreddits:List[str], \n",
    "                             queries:List[str],\n",
    "                             max_requests:int=90, \n",
    "                             min_requests:int=50,\n",
    "                             delay:List[float] = [60.0,120.0],\n",
    "                             **search_kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for parsing functions. Takes a list of subreddits and queries, then calls the parsing \n",
    "    function for each combination of subreddit and query. Submission and comment results from each \n",
    "    inner function call is aggregated and returned as dictionaries for submissions and comments.\n",
    "    \n",
    "    Jitter is implemented to introduce randomness in number of API requests with a short backoff\n",
    "    in each iteration to ensure \n",
    "    \"\"\"\n",
    "    assert isinstance(subreddits, list), \"Argument 'subreddits' expects a list of subreddit names.\"\n",
    "    assert isinstance(queries, list), \"Argument 'queries' expects a list of search queries names.\"\n",
    "    \n",
    "    # Container variables for aggregate data\n",
    "    agg_sub_data: Dict[str, Dict[str, Any]] = {}\n",
    "    agg_comm_data: Dict[str, Dict[str, Any]] = {}\n",
    "    \n",
    "    # API request counter for triggering execution cooldown\n",
    "    trace_requests = []\n",
    "    total_requests = 0\n",
    "    \n",
    "    # Parse submission and comment data with jittered API calls\n",
    "    for subreddit, query in product(subreddits, queries):\n",
    "        # Always ensure that submissions are under max API requests\n",
    "        submission_limit = int(random.uniform(min_requests, max_requests))\n",
    "            \n",
    "        # Calculate estimated requests from current iteration\n",
    "        required_requests = 1 + submission_limit # 1 from search, N for comments from N submissions\n",
    "        \n",
    "        # Check if longer delay should be triggered on this iteration\n",
    "        if total_requests + required_requests > max_requests:\n",
    "            cooldown = random.uniform(*delay)\n",
    "            print(f\"Cooldown triggered: sleeping for {cooldown}s to avoid rate limit.\")\n",
    "            time.sleep(cooldown)\n",
    "            total_requests = 0\n",
    "        \n",
    "        # Extract submissions from each subreddit-query pair\n",
    "        submission_data, comment_data = parse_search_results(**search_kwargs, \n",
    "                                                            subreddit_name=subreddit, \n",
    "                                                            query=query, \n",
    "                                                            limit=submission_limit)\n",
    "        agg_sub_data.update(submission_data)\n",
    "        agg_comm_data.update(comment_data)\n",
    "        total_requests += required_requests\n",
    "        trace_requests.append(total_requests)\n",
    "    \n",
    "    print(f'Execution finished: Total of {sum(trace_requests)} requests made with trace {\"\\n\".join(trace_requests)}.')\n",
    "    return (agg_sub_data, agg_comm_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b06d7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse text files containing search queries and subreddit names\n",
    "search_queries = parse_txt_file(\"../src/search_queries.txt\")\n",
    "subreddits = parse_txt_file(\"../src/subreddits.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9073dfd",
   "metadata": {},
   "source": [
    "### Fetching and parsing search results from Reddit used car communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "193078da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Search Pairs-------\n",
      "CarsAustralia - affordable reliable used cars under 15k Australia\n",
      "CarsAustralia - affordable reliable used cars under 10k USA\n",
      "UsedCars - affordable reliable used cars under 15k Australia\n",
      "UsedCars - affordable reliable used cars under 10k USA\n"
     ]
    }
   ],
   "source": [
    "search_queries = search_queries[:2]\n",
    "subreddits = subreddits[:2]\n",
    "print('-------Search Pairs-------')\n",
    "for (subreddit, query) in product(subreddits, search_queries):\n",
    "    print(subreddit,\"-\",query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "dcb454e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooldown triggered: sleeping for 109.40741414375071s to avoid rate limit.\n",
      "Cooldown triggered: sleeping for 71.57586797057071s to avoid rate limit.\n",
      "Cooldown triggered: sleeping for 65.00197525949584s to avoid rate limit.\n",
      "CPU times: user 2.8 s, sys: 314 ms, total: 3.11 s\n",
      "Wall time: 7min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fetch search results and parse to dict of dicts\n",
    "submission_data, comment_data = aggregate_search_results(subreddits=subreddits, queries=search_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528009c",
   "metadata": {},
   "source": [
    "## Storing the scraped data\n",
    "\n",
    "### Formatting to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "cf8279db",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame.from_dict(submission_data, orient='index')\n",
    "comment_df = pd.DataFrame.from_dict(comment_data, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f22123",
   "metadata": {},
   "source": [
    "### Exporting DataFrame to a Parquet file for efficient storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5c641d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_parquet(os.path.join(\"..\",\"data\",\"submission_data.parquet\"), \n",
    "                         engine='pyarrow',\n",
    "                         compression='gzip')\n",
    "\n",
    "comment_df.to_parquet(os.path.join(\"..\",\"data\",\"comment_data.parquet\"),\n",
    "                      engine='pyarrow',\n",
    "                      compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dba01",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
