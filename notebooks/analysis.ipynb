{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d1e5ad",
   "metadata": {},
   "source": [
    "# Used Car Analysis from Reddit Discussions\n",
    "\n",
    "__Goal:__ Determine ideal used car models for my personal profile by leveraging crowd knowlege.\n",
    "\n",
    "__Objectives:__\n",
    "1. Establish access to Reddit API either via PRAW or Requests\n",
    "2. Extract and parse relevant submissions and respective top-level comments\n",
    "3. Explore and analyze textual data\n",
    "4. Build a recommender that takes into account my personal profile and returns a comprehensive discussion on the ideal used vehicle model/s for me.\n",
    "\n",
    "__Motivation:__ As a first-time car buyer, it's easy to get lost in the sea of options in the used car market. The plethora of options for brands, models, model years and trims, and whatnot, can make it daunting to do your own research. Personally, I find myself gravitating to online forums when doing my research on used cars as I believe in the concept of \"Wisdom of the crowd\", wherein majority sentiment may be indicative of some measurable and verifiable phenomena. \n",
    "\n",
    "In the case of used cars, if a significant number of people praise a specific Japanese-manufactured vehicle model while citing its faults, an algorithm that sufficiently captures this data may be able to present a high confidence level for the said model while presenting an overview of common issues to be aware of. In doing so, the work involved in research is cut in half, and prospective buyers can trim their options down to a few models that fit their criteria, while being made aware of the salient points that require extensive due diligence (e.g. common issues, upkeep, regulations) that only humans can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a7928",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044418d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas praw prawcore python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw, prawcore, time, os, sys, functools, random\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "from collections.abc import Callable, Iterator\n",
    "\n",
    "# Load .env file for access keys\n",
    "load_dotenv(os.path.join('..', 'config', '.env'))\n",
    "\n",
    "# Import config.py to access environment variables\n",
    "sys.path.append('../config')\n",
    "from config import PRAW_ID, PRAW_SECRET, PRAW_USER_AGENT, PRAW_USERNAME, PRAW_PASSWORD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bb44f",
   "metadata": {},
   "source": [
    "## Setting up access to Reddit API\n",
    "Access keys to Reddit API are stored in a .env file under the config directory of this repository. A template for the .env file is provided in the config directory.\n",
    "\n",
    "The config.py script assigns the environment variables to the `PRAW_ID`, `PRAW_SECRET`, `PRAW_USER_AGENT`, `PRAW_USERNAME`, and `PRAW_PASSWORD` global variables respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRAW \n",
    "reddit = praw.Reddit(\n",
    "    client_id = PRAW_ID,\n",
    "    client_secret = PRAW_SECRET,\n",
    "    username = PRAW_USERNAME,\n",
    "    password = PRAW_PASSWORD,\n",
    "    user_agent = PRAW_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01d930",
   "metadata": {},
   "source": [
    "## Extracting text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88863fd",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "The helper functions were designed to extract relevant data and metadata from Reddit submissions and comments, and package the data into a dict of dicts that can be easily parsed into a Pandas DataFrame object for further analysis.\n",
    "\n",
    "`backoff_on_rate_limit`: This is a decorator factory that builds a custom decorator based on specified backoff parameters (max retries, base delay, cap, jitter). The decorator itself is a wrapper for custom functions that call PRAW methods such as `fetch_submissions` and `fetch_comments`, which call subreddit.search() and submission.comments.replace_more() respectively. The decorator implements exponential backoff with optional full jitter to respect Reddit API rate limits while handling transient failures.\n",
    "\n",
    "__Inputs:__\n",
    "- Integer value for max retries. When attempts exceed this number, an Exception is raised\n",
    "- Float for base delay in seconds (i.e. Delay at first failed attempt)\n",
    "- Float for maximum delay in seconds\n",
    "- Bool on whether to implement full jitter or not\n",
    "\n",
    "__Outputs:__\n",
    "- Decorator to be applied to an PRAW API request wrapper\n",
    "\n",
    "`parse_submission_comments`: This is a utility function that fetches comments from a given post and formats each comment as a dictionary of dictionaries with key as comment id and value as a dictionary of comment content and metadata (e.g. body, timestamp, upvotes).\n",
    "\n",
    "__Inputs:__ \n",
    "- Submission object from PRAW (i.e. Reddit posts)\n",
    "- Integer for .replace_more limit parameter, default=0 (i.e. top/parent comments only)\n",
    "\n",
    "__Output:__\n",
    "- Dict of comments in the format {comment_id : {data_header: data_value}}\n",
    "\n",
    "`parse_search_results`: This is a utility function that fetches submissions (posts) from a given subreddit using a predefined search query (i.e. keywords). Submissions are formatted into a dict of dicts with format {submission id : {data_header : data_value}}. This function also wraps the comment fetching function and aggregates the comments from all submissions into a single dict. \n",
    "\n",
    "__Inputs:__ \n",
    "- String of Subreddit name\n",
    "- String of search query\n",
    "- Integer for limit of submissions yielded by PRAW subreddit search\n",
    "\n",
    "__Output:__\n",
    "- Tuple of submissions dict and comments dict\n",
    "\n",
    "__More reading materials:__\n",
    "1. [API Rate Limits Explained: Best Practices for 2025](https://orq.ai/blog/api-rate-limit)\n",
    "2. [Exponential Backoff And Jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8a6d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_on_rate_limit(max_retries:int=5, \n",
    "                        base_delay:float=1.0, \n",
    "                        cap_delay:float=60.0, \n",
    "                        jitter:bool=True) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator factory that applies exponential backoff (with optional jitter)\n",
    "    when Reddit API rate limits (HTTP 429) or server errors occur.\n",
    "    Stops after max_retries and re-raises the exception.\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Start with base delay, then exponentially scale by attempt\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except prawcore.exceptions.ResponseException as e:\n",
    "                    if attempt > max_retries:\n",
    "                        raise Exception(\"Max retries exceeded with Reddit API.\")\n",
    "                    delay = min(cap_delay, base_delay * 2 ** attempt)\n",
    "                    if jitter:\n",
    "                        delay = random.uniform(0, delay)\n",
    "                    print(f\"[WARNING] {e.__class__.__name__} on attempt {attempt+1}, retrying after {delay:.2f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    attempt += 1\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_submissions(subreddit:object, query:str, limit:int=100, **kwargs) -> Iterator:\n",
    "    \"\"\"Modify the subreddit search from PRAw to ensure adherence to safe request limits.\"\"\"\n",
    "    return subreddit.search(**kwargs, query=query, limit=limit)\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_comments(submission:object, limit:int=0) -> list:\n",
    "    \"\"\"Modify the comment fetch from PRAW to ensure adherence to safe request limits.\"\"\"\n",
    "    submission.comments.replace_more(limit=limit)\n",
    "    return submission.comments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_comments(submission:object, limit:int=0):\n",
    "    \"\"\"\n",
    "    Parses comments from a Reddit submission and packages it into a dict of dicts.\n",
    "    Function is called within the main wrapper fetch_search_results.\n",
    "    \"\"\"\n",
    "    # Dict of dicts with format {comment_id : comment_info_dict}\n",
    "    comment_data: Dict[str, Dict[str, Any]] = {}\n",
    "    # Update comments dict with info dict \n",
    "    for comment in fetch_comments(submission, limit=limit):\n",
    "        # Duplicate handling; Skip building throwaway dict\n",
    "        if comment.id in comment_data:\n",
    "            continue\n",
    "        comment_data[comment.id] = {\n",
    "            'body':comment.body,\n",
    "            'score':comment.score,\n",
    "            'timestamp':comment.created_utc,\n",
    "            'subreddit':comment.subreddit_name_prefixed,\n",
    "            'parent_submission_id':submission.id\n",
    "        }\n",
    "    return comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ff8547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_submissions(subreddit_name:str, query:str, limit:int=50, **search_kwargs):\n",
    "    \"\"\"\n",
    "    Fetch submissions/posts given a user-defined search query and returns a tuple of parsed submission\n",
    "    data and Submission objects. The parsed data is a dict of dicts where each key is a submission id \n",
    "    and value is a dict of content and metadata for that particular submission. The object is cached for\n",
    "    subsequent API calls for extracting comments.\n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    # Dict of dicts with format {id : data_dict}\n",
    "    submission_data: Dict[str, Dict[str, Any]] = {}\n",
    "    # List of Submission objects\n",
    "    submissions: List[object] = []\n",
    "    for submission in fetch_submissions(**search_kwargs, subreddit=sub, query=query, limit=limit):\n",
    "        # Duplicate handling; Skip building throwaway dict\n",
    "        if submission.id in submission_data:\n",
    "            continue\n",
    "        # Update submissions dict with info dict from submission\n",
    "        submission_data[submission.id] = {\n",
    "            'title':submission.title,\n",
    "            'selftext':submission.selftext,\n",
    "            'score':submission.score,\n",
    "            'upvote_ratio':submission.upvote_ratio,\n",
    "            'timestamp':submission.created_utc,\n",
    "            'subreddit':submission.subreddit_name_prefixed,\n",
    "            'num_comments':submission.num_comments\n",
    "            }\n",
    "        submissions.append(submission)\n",
    "    return (submission_data, submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35581497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_results(subreddit_name:str, query:str, limit:int=50, **search_kwargs):\n",
    "    \"\"\"\n",
    "    Fetch submissions/posts and respective comments from a subreddit given a user-defined\n",
    "    search query and returns a tuple of relevant data from submissions and comments. Each dataset \n",
    "    is formatted as a dict of dicts in the form {id : {data_header : data_value}} for submissions \n",
    "    and comments.\n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    # Dict of dicts with format {id : data_dict}\n",
    "    submission_data: Dict[str, Dict[str, Any]] = {}\n",
    "    comment_data: Dict[str, Dict[str, Any]] = {}\n",
    "    # Perform subreddit search given query\n",
    "    for submission in fetch_submissions(**search_kwargs, subreddit=sub, query=query, limit=limit):\n",
    "        # Duplicate handling; Skip building throwaway dict\n",
    "        if submission.id in submission_data:\n",
    "            continue\n",
    "        # Update submissions dict with info dict from submission\n",
    "        submission_data[submission.id] = {\n",
    "            'title':submission.title,\n",
    "            'selftext':submission.selftext,\n",
    "            'score':submission.score,\n",
    "            'upvote_ratio':submission.upvote_ratio,\n",
    "            'timestamp':submission.created_utc,\n",
    "            'subreddit':submission.subreddit_name_prefixed,\n",
    "            'num_comments':submission.num_comments\n",
    "            }\n",
    "        # Fetch comments from submission and update comments dict\n",
    "        comment_data.update(parse_comments(submission))\n",
    "    return (submission_data, comment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b0f1e",
   "metadata": {},
   "source": [
    "### Provide an initial list of search queries and Subreddits\n",
    "\n",
    "To scrape the relevant text data from Reddit, I created a small list of queries covering diverse yet relevant topics to buying affordable used vehicles. The queries involved location-specific, model-specific, and thematic keywords to ensure that the search covers as much ground as possible. Chosen subreddits have > 1e5 subscribers to ensure that search queries will yield a significant amount of results per API request.\n",
    "\n",
    "With a 10x10 query and subreddit array, I expect at least an initial 100 requests for the subreddit search yielding 100x50 submissions at most.\n",
    "\n",
    "Fetching the comments involves significantly more requests as each submission requires 1 request to yield the CommentForest. Fetching the comments will require at least 10,000 requests.\n",
    "\n",
    "__Expected Minimum API Requests__\n",
    "|Search Requests|Comment Fetch Requests|Total Requests|\n",
    "|:----------|:----------|:----------|\n",
    "|100      |5,000  |5,100|\n",
    "\n",
    "As such, a single batch job covering all query-subreddit combinations will yield at least 10,100 API requests in a single go, which wildly exceeds the Reddit API fair use policy (i.e. Cap requests to 100/min averaged over 10-minute sliding window). To address this issue, batched processing will be implemented to ensure average requests is under safe rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da66db6",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "`parse_txt_file`: Parses text files containing data separated by newlines. Returns a list. Used for containerizing search_queries and subreddit strings into separate text files that can be easily mutated without modifying source code.\n",
    "\n",
    "__Input:__\n",
    "- String for the path of text file, with each item separated by a newline\n",
    "\n",
    "__Output:__\n",
    "- List (e.g. search queries, subreddit names)\n",
    "\n",
    "`aggregate_search_results`: This function wraps the parse_search_results and feeds a combination of subreddit and query arguments from a list of subreddit and queries. It returns a tuple of dictionaries, similar to the parse_search_results function, but aggregated over all submissions and comments.\n",
    "\n",
    "__Inputs:__ \n",
    "- List of subreddit name strings\n",
    "- List of search query strings\n",
    "\n",
    "__Output:__\n",
    "- Tuple of aggregated submissions dict and comments dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "48b8744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_file(file_path:str):\n",
    "    \"\"\"\n",
    "    Utility function for parsing a multi-line text file where each item is separated\n",
    "    by a newline.\n",
    "    Input: String for file path\n",
    "    Output: List\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Ignore comments and empty lines\n",
    "        results = [line.rstrip(\"\\n\") for line in f if not (line.startswith('#') or line.startswith(\"\\n\"))]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_search_results(subreddits:List[str], \n",
    "                             queries:List[str],\n",
    "                             submission_limit:int, \n",
    "                             max_requests:int=90, \n",
    "                             min_requests:int=50,\n",
    "                             jitter:bool=True, \n",
    "                             **search_kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for fetch_search_results func that takes a list of subreddits and queries, then\n",
    "    calls the fetch function for each combination of subreddit and query. Submission and comment\n",
    "    results from each inner function call is aggregated and returned as respective aggregate\n",
    "    dictionaries for submissions and comments.\n",
    "    \"\"\"\n",
    "    agg_submission_data: Dict[str, Dict[str, Any]] = {}\n",
    "    agg_comment_data: Dict[str, Dict[str, Any]] = {}\n",
    "    \n",
    "    # Implement jitter to remove predictability\n",
    "    if jitter:\n",
    "        max_requests = max(min_requests, random.uniform(min_requests, max_requests)//1)\n",
    "    \n",
    "    for subreddit, query in zip(subreddits, queries):\n",
    "        # Extract submissions and comments data from search results\n",
    "        submission_data, comment_data = parse_search_results(**search_kwargs, subreddit_name=subreddit, query=query, limit=submission_limit)\n",
    "        # Update aggregate dictionaries with data from each subreddit query\n",
    "        agg_submission_data.update(submission_data)\n",
    "        agg_comment_data.update(comment_data)\n",
    "    return (agg_submission_data, agg_comment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9cf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_results(subreddits:List[str], \n",
    "                             queries:List[str],\n",
    "                             submission_limit:int=50, \n",
    "                             max_requests:int=90, \n",
    "                             min_requests:int=50,\n",
    "                             jitter:bool=True, \n",
    "                             short_delay:List[int]=[0,1],\n",
    "                             long_delay:List[float] = [60.0,120.0],\n",
    "                             **search_kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for parsing functions. Takes a list of subreddits and queries, then calls the parsing \n",
    "    function for each combination of subreddit and query. Submission and comment results from each \n",
    "    inner function call is aggregated and returned as dictionaries for submissions and comments.\n",
    "    \n",
    "    Jitter is implemented to introduce randomness in number of API requests with a short backoff\n",
    "    in each iteration to ensure \n",
    "    \"\"\"\n",
    "    assert isinstance(subreddits, list), \"Argument 'subreddits' expects a list of subreddit names.\"\n",
    "    assert isinstance(queries, list), \"Argument 'queries' expects a list of search queries names.\"\n",
    "    \n",
    "    # Container variables for aggregate data\n",
    "    agg_sub_data: Dict[str, Dict[str, Any]] = {}\n",
    "    agg_comm_data: Dict[str, Dict[str, Any]] = {}\n",
    "    \n",
    "    # Parse submission and comment data with jittered API calls\n",
    "    for subreddit, query in zip(subreddits, queries):\n",
    "        # Always ensure that submissions are under max API requests\n",
    "        if jitter:\n",
    "            submission_limit = max(min_requests, random.uniform(max_requests))\n",
    "        else:\n",
    "            submission_limit = min(submission_limit, max_requests)\n",
    "        \n",
    "        # Extract submissions from each subreddit-query pair\n",
    "        submission_data, submissions = parse_submissions(**search_kwargs, \n",
    "                                                         subreddit_name=subreddit, \n",
    "                                                         query=query, \n",
    "                                                         limit=submission_limit)\n",
    "        agg_sub_data.update(submission_data)\n",
    "        \n",
    "        # Extract comments from each submission\n",
    "        for submission in submissions:\n",
    "            comment_data = parse_comments(submission)\n",
    "            agg_comm_data.update(comment_data)\n",
    "            # Short delay in extracting comments between each submission\n",
    "            time.sleep(random.uniform(*short_delay))\n",
    "        \n",
    "        # Longer delay after each subreddit-query pair\n",
    "        time.sleep(random.uniform(*long_delay))\n",
    "    \n",
    "    return (agg_sub_data, agg_comm_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "fb300eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.0"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(random.uniform(50,90)//1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse text files containing search queries and subreddit names\n",
    "search_queries = parse_txt_file(\"../src/search_queries.txt\")\n",
    "subreddits = parse_txt_file(\"../src/subreddits.txt\")\n",
    "\n",
    "# Convert to iterator\n",
    "search_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9073dfd",
   "metadata": {},
   "source": [
    "### Fetching and parsing search results from Reddit used car communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb454e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_query = search_queries[0]\n",
    "sample_subreddit = subreddits[0]\n",
    "\n",
    "# Fetch search results\n",
    "# ~101 API requests for limit=100; 1 search request, 100 comment fetch requests\n",
    "submission_data, comment_data = fetch_search_results(subreddit_name=sample_subreddit, query=sample_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
