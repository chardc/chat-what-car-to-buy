{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d1e5ad",
   "metadata": {},
   "source": [
    "# Data Pipeline Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a7928",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044418d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas praw prawcore python-dotenv pyarrow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c3309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw, prawcore, praw.models\n",
    "import random, time, os, sys, functools, json, datetime as dt\n",
    "import pyarrow as pa, pyarrow.parquet as pq, pyarrow.compute as pc\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional, List, Dict, Any, Union, Tuple, NamedTuple, Literal\n",
    "from itertools import product\n",
    "from collections import deque, namedtuple\n",
    "from collections.abc import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bb44f",
   "metadata": {},
   "source": [
    "## Setting up access to Reddit API\n",
    "Access keys to Reddit API are stored in a .env file under the config directory of this repository. A template for the .env file is provided in the config directory.\n",
    "\n",
    "The config.py script assigns the environment variables to the `PRAW_ID`, `PRAW_SECRET`, `PRAW_USER_AGENT`, `PRAW_USERNAME`, and `PRAW_PASSWORD` global variables respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ec55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file for access keys\n",
    "module_path = '../src'\n",
    "load_dotenv(os.path.join(module_path,'usedcaranalytics','config','.env'))\n",
    "\n",
    "# Import config.py to access environment variables\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from usedcaranalytics.config.api import PRAW_ID, PRAW_SECRET, PRAW_USER_AGENT, PRAW_USERNAME, PRAW_PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3a53ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded API keys and login credentials.\n"
     ]
    }
   ],
   "source": [
    "if all((PRAW_ID, PRAW_SECRET, PRAW_USER_AGENT, PRAW_USERNAME, PRAW_PASSWORD)):\n",
    "    print('Successfully loaded API keys and login credentials.')\n",
    "else:\n",
    "    raise Exception('Reddit API keys and login credentials unsuccessfully loaded. Retry the script.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474a926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRAW \n",
    "REDDIT = praw.Reddit(\n",
    "    client_id=PRAW_ID,\n",
    "    client_secret=PRAW_SECRET,\n",
    "    username=PRAW_USERNAME,\n",
    "    password=PRAW_PASSWORD,\n",
    "    user_agent=PRAW_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01d930",
   "metadata": {},
   "source": [
    "## Extracting text data\n",
    "\n",
    "This section deals with the process of extracting and storing text data and metadata from Reddit posts and comments. My objective is to present my thought process and design principles in implementing the data pipeline for this project.\n",
    "\n",
    "__Data Pipeline Overview:__\n",
    "1. Establish access to Reddit API\n",
    "2. Crawl predefined subreddits by searching submissions using predefined queries\n",
    "3. Extract textual data and metadata from relevant posts and child comments\n",
    "4. Preprocess data (Optional)\n",
    "5. Store extracted data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ca8f5",
   "metadata": {},
   "source": [
    "### The Challenge of Reddit API Rate Limiting\n",
    "\n",
    "__Building the search query and subreddit pairs__\n",
    "\n",
    "To scrape the relevant text data from Reddit, I created a small list of queries covering diverse yet relevant topics to buying affordable used vehicles. The queries involved location-specific, model-specific, and thematic keywords to ensure that the search covers as much ground as possible. Chosen subreddits have > 1e5 subscribers to ensure that search queries will yield a significant amount of results per API request. These queries and subreddits can be accessed in the paths: `../src/search_queries.txt` and `../src/subreddits.txt`, respectively. \n",
    "\n",
    "__Searching relevant posts per Subreddit__\n",
    "\n",
    "The objective is to search and scrape for posts (and child comments) within the specified subreddits using the search queries provided. However, with a 10x10 query and subreddit array, I expect at least an initial 100 requests for the subreddit search yielding 100x100 submissions at most. Fetching the comments involves significantly more requests as each submission requires 1 request to yield the CommentForest. Fetching the comments will require at least 10,000 requests.\n",
    "\n",
    "__Expected Minimum API Requests__\n",
    "|Search Requests|Comment Fetch Requests|Total Requests|\n",
    "|:----------|:----------|:----------|\n",
    "|100      |10,000  |10,100|\n",
    "\n",
    "From the table above, a single batch job covering all query-subreddit combinations will yield at least 10,100 API requests in a single go, which wildly exceeds the Reddit API fair use policy (i.e. Cap requests to 100/min averaged over 10-minute sliding window). \n",
    "\n",
    "__Implementing a sliding window request counter and backoff algorithms__\n",
    "\n",
    "To ensure the script adheres to fair use policies, I implemented two-pronged fail-safe logic:\n",
    "1. Handle transient failures for each API request by implementing a backoff algorithm\n",
    "2. Mitigate the risk of #1 happening by implementing a program-level API request counter that tracks current and expected calls within a specified sliding window. This rate limiter will throttle requests until there's an available slot.\n",
    "\n",
    "__Read more:__\n",
    "1. [API Rate Limits Explained: Best Practices for 2025](https://orq.ai/blog/api-rate-limit)\n",
    "2. [Exponential Backoff And Jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)\n",
    "3. [Yield Statements vs. Returning Lists in Python](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://community.aws/content/2h01Byx1ytU8357tp2bvcUuJ2j0/yield-statements-vs-returning-lists-in-python%23:~:text%3DYield%253A%2520Ideal%2520for%2520large%2520data,potentially%2520leading%2520to%2520memory%2520errors.&ved=2ahUKEwjzvJvd74uOAxVkQ6QEHVAVMHcQFnoECBIQAw&usg=AOvVaw3hMoJHnPwBIQOdBmB_NiBD)\n",
    "4. [Rate Limiter - Sliding Window Counter](https://medium.com/@avocadi/rate-limiter-sliding-window-counter-7ec08dbe21d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389816fa",
   "metadata": {},
   "source": [
    "##### Rate Limiter Class\n",
    "\n",
    "The RateLimiter class is initialized at the beginning of the script and is used to track API requests made within a specific sliding window. Requests are throttled when total expected requests go beyond rate limits (Reddit = 100/min) for the current window. Jitter is injected to the wait time and a random buffer for requests is left to avoid coasting at rate limits (abuse avoidance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ed54127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRAW auth limits checker implementation\n",
    "\n",
    "class RateLimiter:\n",
    "    MAX_REQUESTS = 1000\n",
    "    PERIOD = 600.0\n",
    "    \n",
    "    def __init__(self, reddit:praw.Reddit, buffer_range:Union[Tuple[int, int], List[int]]=(50,100)):\n",
    "        self.reddit = reddit\n",
    "        self.buffer_range = buffer_range\n",
    "        self.total_requests = 0\n",
    "        self.requests_in_window = deque()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"Checks if current request can be accommodated based on current limits.\"\"\"        \n",
    "        remaining_requests = self.reddit.auth.limits['remaining']\n",
    "        \n",
    "        # If remaining requests from praw.Reddit.auth.limits unavailable (no requests yet),\n",
    "        # return manually counted results from sliding window counter\n",
    "        if remaining_requests is None:\n",
    "            self._refresh_window()\n",
    "            remaining_requests = self.MAX_REQUESTS - len(self.requests_in_window)\n",
    "        \n",
    "        buffer = random.randint(*self.buffer_range)\n",
    "        \n",
    "        # If we dip into the buffer, sleep until limits reset\n",
    "        if remaining_requests - 1 < buffer:\n",
    "            # Calculate time left until limits refresh and add jitter\n",
    "            reset_time = self.reddit.auth.limits['reset_timestamp']\n",
    "            \n",
    "            # If Praw returns no information, then manually compute request reset time\n",
    "            if reset_time is None:\n",
    "                # Manually get the reset time, which is at least 600 seconds from earliest request in window\n",
    "                reset_time = self.requests_in_window[0] + self.PERIOD\n",
    "            \n",
    "            delay = max(reset_time - time.time(), 0) + random.randrange(0.01, 5.0)\n",
    "            time.sleep(delay)\n",
    "            # Re-evaluate if API call can proceed\n",
    "            return self.evaluate()\n",
    "        \n",
    "        # Update sliding window counter\n",
    "        self._refresh_window()._log_request()\n",
    "        \n",
    "        # Tally API call\n",
    "        self.total_requests += 1\n",
    "    \n",
    "    def _log_request(self):\n",
    "        \"\"\"\n",
    "        Failsafe for when REDDIT.auth.limits is unavailable. Tracks the requests_in_window attribute, which\n",
    "        is a deque containing timestamps of API requests made in the current sliding window (600 seconds).\n",
    "        \n",
    "        Usage: \n",
    "            - Before every API call, refresh the sliding window and append current timestamp to mark \n",
    "            outbound request via \"self._refresh_window()._log_request()\".\n",
    "        \"\"\"\n",
    "        self.requests_in_window.append(time.time())\n",
    "        return self\n",
    "    \n",
    "    def _refresh_window(self):\n",
    "        \"\"\"\n",
    "        Updates self.requests_in_window by evaluating the number of requests in current window. Removes\n",
    "        request logs outside current 10-minute sliding window.\n",
    "        \"\"\"\n",
    "        # Remove requests outside 600 second sliding window (i.e. earlier than 600s ago)\n",
    "        while self.requests_in_window and self.requests_in_window[0] < time.time() - self.PERIOD:\n",
    "            self.requests_in_window.popleft()\n",
    "        return self\n",
    "    \n",
    "    def print_total_requests(self):\n",
    "        return f'{self.total_requests} total requests as of {dt.datetime.now():%Y-%m-%d %H:%M:%S}.'\n",
    "    \n",
    "    def print_remaining_requests(self):\n",
    "        \"\"\"\n",
    "        Prints the number of remaining requests in current window either based on PRAW reddit.auth.limits\n",
    "        or manual sliding window counter.\n",
    "        \"\"\"\n",
    "        limits = self.reddit.auth.limits\n",
    "        # If limits hasn't refreshed yet, return remaining requests. Otherwise, return total limit\n",
    "        if all(limits['reset_timestamp'], limits['remaining_requests'], limits['used']):\n",
    "            return f'{limits['remaining_requests']}' \\\n",
    "                if time.time() < limits['reset_timestamp'] \\\n",
    "                else f'{limits['remaining_requests'] + limits['used']}'\n",
    "        # Refresh current window and return difference between max requests and # requests in window\n",
    "        else:\n",
    "            self._refresh_window()\n",
    "            return f'{self.MAX_REQUESTS - len(self.requests_in_window)}'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'RateLimiter object: {self.print_remaining_requests()} available requests in current window. \\\n",
    "            {self.print_total_requests()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda2edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize rate limiter\n",
    "rate_limiter = RateLimiter(REDDIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3feb418",
   "metadata": {},
   "source": [
    "##### Decorator Factory for implementing Exponential Backoff and Full Jitter\n",
    "The function below creates a flexible decorator that can be adjusted based on the intended maximum retries, exponential backoff caps, and inclusion of jitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_on_rate_limit(max_retries:int=5, \n",
    "                        base_delay:float=1.0, \n",
    "                        cap_delay:float=60.0, \n",
    "                        jitter:bool=True):\n",
    "    \"\"\"\n",
    "    Decorator factory that applies exponential backoff (with optional jitter) when Reddit API\n",
    "    rate limits (HTTP 429) or server errors occur. Stops after max_retries and re-raises the exception.\n",
    "    \n",
    "    Input:\n",
    "        - Integer value for max retries. When attempts exceed this number, an Exception is raised\n",
    "        - Float for base delay in seconds (i.e. Delay at first failed attempt)\n",
    "        - Float for maximum delay in seconds\n",
    "        - Bool on whether to implement full jitter or not\n",
    "    Output:\n",
    "        - Decorator to be applied to an PRAW API request wrapper\n",
    "    \"\"\"\n",
    "    def decorator(func):# -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]:\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Start with base delay, then exponentially scale by attempt\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except prawcore.exceptions.ResponseException as e:\n",
    "                    if attempt > max_retries:\n",
    "                        raise Exception(\"Max retries exceeded with Reddit API.\")\n",
    "                    delay = min(cap_delay, base_delay * 2 ** attempt)\n",
    "                    if jitter:\n",
    "                        delay = random.uniform(0, delay)\n",
    "                    print(f\"[WARNING] {e.__class__.__name__} on attempt {attempt+1}, retrying after {delay:.2f}s.\")\n",
    "                    time.sleep(delay)\n",
    "                    attempt += 1\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d4d07",
   "metadata": {},
   "source": [
    "#### API Call Wrappers with Backoff Algorithms\n",
    "\n",
    "The helper functions were designed to extract relevant data and metadata from Reddit submissions and comments, and package the data into a dict of dicts that can be easily parsed into a Pandas DataFrame object for further analysis. The backoff decorator is applied to each API call wrapper to handle transient errors raised by HTTP 429 response (Too Many Requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05846dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff_on_rate_limit()\n",
    "def fetch_submissions(subreddit:object, query:str, limit:int=100, **kwargs):\n",
    "    \"\"\"Modify the subreddit search from PRAw to ensure adherence to safe request limits.\"\"\"\n",
    "    # Record API request and throttle if needed\n",
    "    rate_limiter.evaluate()\n",
    "    return subreddit.search(**kwargs, query=query, limit=limit)\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_comments(submission:object, limit:int=0):\n",
    "    \"\"\"Modify the comment fetch from PRAW to ensure adherence to safe request limits.\"\"\"\n",
    "    # Record API request and throttle if needed\n",
    "    rate_limiter.evaluate()\n",
    "    # Replace 'more' with specified limit (default = 0 or retain top-level comments only)\n",
    "    submission.comments.replace_more(limit=limit)\n",
    "    for comment in submission.comments:\n",
    "        yield comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d0775",
   "metadata": {},
   "source": [
    "#### Text File Parser for Subreddit and Search Queries\n",
    "\n",
    "To limit hardcoding and allow for flexible scraping, I opted to store the list of subreddit names and search queries in their individual text files so that I can simply update that file and re-run the script whenever I need to scrape something rather than digging through the source code whenever I need to modify the search pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b7a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_file(file_path:str):\n",
    "    \"\"\"\n",
    "    Utility function for parsing a multi-line text file where each item is separated\n",
    "    by a newline.\n",
    "    \n",
    "    Input:\n",
    "        - String for the path of text file, with each item separated by a newline\n",
    "    Output:\n",
    "        - List (e.g. search queries, subreddit names)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Ignore comments and empty lines\n",
    "        results = [line.rstrip(\"\\n\") for line in f if not (line.startswith('#') or line.startswith(\"\\n\"))]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f7a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_pairs(\n",
    "    project_root:str, \n",
    "    subdir:str='data/raw', \n",
    "    file_names:Union[Tuple[str],List[str]]=('search_queries.txt','subreddits.txt')\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Utility function that returns tuple of lists of subreddit names and search queries.\n",
    "    Input:\n",
    "        - project root path\n",
    "        - subdirectory path that contains the files\n",
    "        - file names in strict order: 1) search queries, 2) subreddits\n",
    "    Output:\n",
    "        - Tuple(list of queries, list of subreddits)\n",
    "    \"\"\"\n",
    "    \n",
    "    query_fp, subreddit_fp = tuple(\n",
    "        os.path.join(project_root, subdir, fname) for fname in file_names\n",
    "        )\n",
    "    \n",
    "    return tuple(parse_txt_file(fp) for fp in (query_fp, subreddit_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2dc7a",
   "metadata": {},
   "source": [
    "### Data Streaming\n",
    "\n",
    "__Rationale: Scalability of Scraping Logic__\n",
    "\n",
    "Previously, I explored building dictionaries within each scraping function and returning that dictionary to the data storage logic. However, this approach doesn't scale well since device memory may become a bottleneck with larger volumes of API calls. From my research, it's recommended to use generators to stream data from APIs as memory overhead is limited to the data extracted from the most recent call.\n",
    "\n",
    "__Data Extraction Overview__:\n",
    "\n",
    "1. Initialize the rate limiter class to keep track of requests within a 60-second sliding window.\n",
    "2. Parse the text files containing subreddit names and search queries, then get the combination of search pairs.\n",
    "3. Given a search pair, initialize a Subreddit class and search relevant submissions within that subreddit.\n",
    "4. For every relevant submission, extract relevant data from Submission class attributes and stream a tuple of record type and submission data dictionary\n",
    "5. Subsequently, for every submission, request the top-level comments, and for each top-level comment, stream a tuple of record type and comment data dictionary.\n",
    "\n",
    "#### Data Streaming Functions\n",
    "1. Streamer for comments given a praw.models.Submission object.\n",
    "2. Streamer for submission data and child comments. This is a wrapper for the comment streamer that takes a single search pair as input, streams all relevant submissions and child comments.\n",
    "3. Wrapper for #2 and takes a list of subreddits and search queries and inputs the search pair combinations into the submission and comment streaming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf4f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_comments(submission:object, limit:int=0):\n",
    "    \"\"\"\n",
    "    Fetches comments from a Submission objects then parses each comment into a dictionary record.\n",
    "    Each entry is streamed for efficient memory footprint when handling larger CommentForests.\n",
    "    \n",
    "    Input:\n",
    "        - Submission object from PRAW (i.e. Reddit posts)\n",
    "        - Integer for .replace_more limit parameter, default=0 (i.e. top/parent comments only)\n",
    "    Output:\n",
    "        - Dict of comments in the format {comment_id : {data_header: data_value}}  \n",
    "    \"\"\"\n",
    "    assert (isinstance(limit, int) and limit >= 0) or limit is None, 'Limit must be an integer >= 0 or None.'\n",
    "    \n",
    "    # Update comments dict with info dict \n",
    "    for comment in fetch_comments(submission, limit=limit):\n",
    "        \n",
    "        # Stream comment data when slot available in current window\n",
    "        yield \"comment\", {\n",
    "            'comment_id':comment.id,\n",
    "            'body':comment.body,\n",
    "            'score':comment.score,\n",
    "            'timestamp':int(comment.created_utc),\n",
    "            'subreddit':comment.subreddit_name_prefixed,\n",
    "            'parent_submission_id':submission.id\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_submissions_and_comments(subreddit_name:str, query:str, limit:int=50, **search_kwargs):\n",
    "    \"\"\"\n",
    "    Fetches submissions, parses each submission into a dictionary record, and calls the stream_comments\n",
    "    function on each submission. Submission data and comment data are streamed for efficient memory \n",
    "    footprint when handling larger datasets. \n",
    "    \n",
    "    Input:\n",
    "        - String of Subreddit name\n",
    "        - String of search query\n",
    "        - Integer for limit of submissions yielded by PRAW subreddit search\n",
    "    Output:\n",
    "        - Tuple of submission data dict and comment data dict\n",
    "    \"\"\"\n",
    "    assert isinstance(subreddit_name, str), 'Subreddit name must be a string.'\n",
    "    assert isinstance(query, str), 'Search query must be a string.'\n",
    "    assert isinstance(limit, int) and limit > 0, 'Limit must be a positive non-zero integer.'\n",
    "    \n",
    "    sub = REDDIT.subreddit(subreddit_name)\n",
    "    \n",
    "    # Fetch submissions, and for every submission, fetch the comments\n",
    "    for submission in fetch_submissions(**search_kwargs, subreddit=sub, query=query, limit=limit):\n",
    "        # Stream comment data from current submission (\"submission\", Dict[str, Any])\n",
    "        yield from stream_comments(submission)\n",
    "        \n",
    "        # Stream submission data when slot available in current window\n",
    "        yield \"submission\", {\n",
    "            'submission_id':submission.id,\n",
    "            'title':submission.title,\n",
    "            'selftext':submission.selftext,\n",
    "            'score':submission.score,\n",
    "            'upvote_ratio':submission.upvote_ratio,\n",
    "            'timestamp':int(submission.created_utc),\n",
    "            'subreddit':submission.subreddit_name_prefixed,\n",
    "            'num_comments':submission.num_comments\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de9cf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_aggregate_results(subreddits:List[str], queries:List[str],**search_kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for streaming functions. Takes a list of subreddits and queries, then calls the \n",
    "    stream_search_results  function for each combination of subreddit and query. \n",
    "    \n",
    "    Input:\n",
    "        - List of subreddit name strings\n",
    "        - List of search query strings\n",
    "        - Int of maximum requests per minute, also determines upper bound of search result limit\n",
    "        - Int of minimum requests, which is the floor of search result limit\n",
    "        - Float of seconds denoting the time period for counting the API call limits\n",
    "        - List of float values of seconds to randomly add to interval delay \n",
    "    Output:\n",
    "        - Tuple of aggregated submissions dict and comments dict\n",
    "    \"\"\"\n",
    "    assert isinstance(subreddits, list), \"Argument 'subreddits' expects a list of subreddit names.\"\n",
    "    assert isinstance(queries, list), \"Argument 'queries' expects a list of search queries names.\"\n",
    "    \n",
    "    # Parse submission and comment data with jittered API calls\n",
    "    for subreddit, query in product(subreddits, queries):\n",
    "        # Stream submission and comment records (str(record_type), Dict[str(col_name), Any])\n",
    "        yield from stream_submissions_and_comments(**search_kwargs, subreddit_name=subreddit, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d7dea",
   "metadata": {},
   "source": [
    "#### Testing the streaming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e580bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the streamer\n",
    "sample_stream = tuple(\n",
    "    stream_aggregate_results(\n",
    "        ['cars'],\n",
    "        ['toyota corolla first car'],\n",
    "        limit=10\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47ca44b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11 total requests as of 2025-06-29 16:42:43.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print total requests made so far\n",
    "rate_limiter.print_total_requests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daed3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregate data then build dataframes\n",
    "comment_data = tuple(data for record, data in sample_stream if record == 'comment')\n",
    "submission_data = tuple(data for record, data in sample_stream if record == 'submission')\n",
    "comment_df = pd.DataFrame(comment_data)\n",
    "submission_df = pd.DataFrame(submission_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f67c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>parent_submission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ioe046j</td>\n",
       "      <td>The GR is going to help Toyota sell the regula...</td>\n",
       "      <td>164</td>\n",
       "      <td>1663158970</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iodt8kt</td>\n",
       "      <td>Lots of talk about it being 'safe' and 'stable...</td>\n",
       "      <td>95</td>\n",
       "      <td>1663155458</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iodw9bg</td>\n",
       "      <td>Will be interesting to see if this car or any ...</td>\n",
       "      <td>68</td>\n",
       "      <td>1663157124</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iodun0d</td>\n",
       "      <td>Now I’m ready for the Civic Type R, WRX, Elant...</td>\n",
       "      <td>101</td>\n",
       "      <td>1663156249</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ioenkne</td>\n",
       "      <td>A $50k crazy Corolla is cool and all, but can'...</td>\n",
       "      <td>48</td>\n",
       "      <td>1663168836</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id                                               body  score  \\\n",
       "0    ioe046j  The GR is going to help Toyota sell the regula...    164   \n",
       "1    iodt8kt  Lots of talk about it being 'safe' and 'stable...     95   \n",
       "2    iodw9bg  Will be interesting to see if this car or any ...     68   \n",
       "3    iodun0d  Now I’m ready for the Civic Type R, WRX, Elant...    101   \n",
       "4    ioenkne  A $50k crazy Corolla is cool and all, but can'...     48   \n",
       "\n",
       "    timestamp subreddit parent_submission_id  \n",
       "0  1663158970    r/cars               xdyy00  \n",
       "1  1663155458    r/cars               xdyy00  \n",
       "2  1663157124    r/cars               xdyy00  \n",
       "3  1663156249    r/cars               xdyy00  \n",
       "4  1663168836    r/cars               xdyy00  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(comment_df.shape)\n",
    "comment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e087ebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xdyy00</td>\n",
       "      <td>Toyota GR Corolla | First Drive, Leveling Expe...</td>\n",
       "      <td></td>\n",
       "      <td>433</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1663153268</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51ej3q</td>\n",
       "      <td>You are sent back in time to when Henry Ford i...</td>\n",
       "      <td>Id send him a 1999 Toyota Levin with 400,000km...</td>\n",
       "      <td>539</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1473157092</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qpinpq</td>\n",
       "      <td>Toyota has some huge potential with the GR Cor...</td>\n",
       "      <td>For the first time in my life, I think I will ...</td>\n",
       "      <td>386</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1636393029</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15zrb24</td>\n",
       "      <td>Is it just me or is the Toyota corolla the mos...</td>\n",
       "      <td>So I today I had the (dis)pleasure of renting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1692851066</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x64gh1</td>\n",
       "      <td>2023 Toyota GR Corolla - Circuit Edition vs. M...</td>\n",
       "      <td></td>\n",
       "      <td>256</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1662344567</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id                                              title  \\\n",
       "0        xdyy00  Toyota GR Corolla | First Drive, Leveling Expe...   \n",
       "1        51ej3q  You are sent back in time to when Henry Ford i...   \n",
       "2        qpinpq  Toyota has some huge potential with the GR Cor...   \n",
       "3       15zrb24  Is it just me or is the Toyota corolla the mos...   \n",
       "4        x64gh1  2023 Toyota GR Corolla - Circuit Edition vs. M...   \n",
       "\n",
       "                                            selftext  score  upvote_ratio  \\\n",
       "0                                                       433          0.93   \n",
       "1  Id send him a 1999 Toyota Levin with 400,000km...    539          0.87   \n",
       "2  For the first time in my life, I think I will ...    386          0.89   \n",
       "3  So I today I had the (dis)pleasure of renting ...      0          0.31   \n",
       "4                                                       256          0.92   \n",
       "\n",
       "    timestamp subreddit  num_comments  \n",
       "0  1663153268    r/cars           251  \n",
       "1  1473157092    r/cars           545  \n",
       "2  1636393029    r/cars           244  \n",
       "3  1692851066    r/cars           183  \n",
       "4  1662344567    r/cars           112  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(submission_df.shape)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74c3e8",
   "metadata": {},
   "source": [
    "#### Consolidating the streaming functions into a DataStreamer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07579db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStreamer:\n",
    "    def __init__(self, reddit:praw.Reddit):\n",
    "        self.reddit = reddit\n",
    "        self.rate_limiter = RateLimiter(reddit)\n",
    "    \n",
    "    @backoff_on_rate_limit()\n",
    "    def _fetch_submissions(self, subreddit:praw.models.Subreddit, query:str, **kwargs):\n",
    "        \"\"\"Modify the subreddit search from PRAw to ensure adherence to safe request limits.\"\"\"\n",
    "        # Evaluate if current request can be accommodated with remaining limits\n",
    "        self.rate_limiter.evaluate()\n",
    "        return subreddit.search(**kwargs, query=query)\n",
    "\n",
    "    @backoff_on_rate_limit()\n",
    "    def _fetch_comments(self, submission:praw.models.Submission, **kwargs):\n",
    "        \"\"\"Modify the comment fetch from PRAW to ensure adherence to safe request limits.\"\"\"\n",
    "        # Evaluate if current request can be accommodated with remaining limits\n",
    "        self.rate_limiter.evaluate()\n",
    "        # Replace 'more comments' with specified limit (default = 0 or retain top-level comments only)\n",
    "        submission.comments.replace_more(**kwargs)\n",
    "        yield from submission.comments\n",
    "    \n",
    "    def _stream_comments(\n",
    "        self, \n",
    "        submission:praw.models.Submission,\n",
    "        **kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Fetches comments from a Submission objects then parses each comment into a dictionary record.\n",
    "        Each entry is streamed for efficient memory footprint when handling larger CommentForests.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generator\n",
    "        comments = self._fetch_comments(submission, **kwargs)\n",
    "        \n",
    "        # Update comments dict with info dict \n",
    "        for comment in comments:            \n",
    "            \n",
    "            # Stream comment data when slot available in current window\n",
    "            yield \"comment\", {\n",
    "                'comment_id':comment.id,\n",
    "                'body':comment.body,\n",
    "                'score':comment.score,\n",
    "                'timestamp':int(comment.created_utc),\n",
    "                'subreddit':comment.subreddit_name_prefixed,\n",
    "                'parent_submission_id':submission.id\n",
    "                }\n",
    "    \n",
    "    def stream_search_results(\n",
    "        self, \n",
    "        subreddit_name:str, \n",
    "        query:str, \n",
    "        limit:int=50,\n",
    "        progress_bar:bool=None, \n",
    "        **search_kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Fetches submissions then fetches the comments for each submission. Data is then repackaged \n",
    "        into a dictionary and streamed as (str(record type), Dict[str(column name), Any(data)]).\n",
    "        \"\"\"\n",
    "        subreddit = self.reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Iterator\n",
    "        submissions = self._fetch_submissions(**search_kwargs, subreddit=subreddit, query=query, limit=limit)\n",
    "        \n",
    "        if progress_bar:\n",
    "            submissions = tqdm(\n",
    "                submissions,\n",
    "                total=limit,\n",
    "                position=1,\n",
    "                colour='red',\n",
    "                desc=f\"fetching submissions...\",\n",
    "                unit='posts',\n",
    "                leave=False\n",
    "            )\n",
    "        \n",
    "        # Fetch submissions, and for every submission, fetch the comments\n",
    "        for submission in submissions:\n",
    "            \n",
    "            # Stream comment data from current submission (\"submission\", Dict[str, Any])\n",
    "            yield from self._stream_comments(submission, limit=0)\n",
    "            \n",
    "            # Stream submission data when slot available in current window\n",
    "            yield \"submission\", {\n",
    "                'submission_id':submission.id,\n",
    "                'title':submission.title,\n",
    "                'selftext':submission.selftext,\n",
    "                'score':submission.score,\n",
    "                'upvote_ratio':submission.upvote_ratio,\n",
    "                'timestamp':int(submission.created_utc),\n",
    "                'subreddit':submission.subreddit_name_prefixed,\n",
    "                'num_comments':submission.num_comments\n",
    "                }\n",
    "            \n",
    "    def stream(\n",
    "        self, \n",
    "        subreddits:List[str], \n",
    "        queries:List[str], \n",
    "        progress_bar:bool=None, \n",
    "        **search_kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Wrapper for streaming functions. Takes a list of subreddits and queries, then calls the \n",
    "        stream_search_results method for each combination of subreddit and query. \n",
    "        \"\"\"\n",
    "        # Generator of combinations\n",
    "        search_pairs = product(subreddits, queries)\n",
    "        \n",
    "        # List\n",
    "        if progress_bar:\n",
    "            search_pairs = list(search_pairs)\n",
    "            search_pairs = tqdm(\n",
    "                search_pairs, \n",
    "                total=len(search_pairs),\n",
    "                desc=f'streaming subreddit search results...',\n",
    "                unit='queries',\n",
    "                colour='orange'\n",
    "                )\n",
    "        \n",
    "        # Parse submission and comment data with jittered API calls\n",
    "        for subreddit, query in search_pairs:\n",
    "            \n",
    "            # Stream submission and comment records (str(record_type), Dict[str(col_name), Any])\n",
    "            yield from self.stream_search_results(\n",
    "                **search_kwargs, \n",
    "                subreddit_name=subreddit, \n",
    "                query=query, \n",
    "                progress_bar=progress_bar\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c5c3e",
   "metadata": {},
   "source": [
    "#### Test the DataStreamer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c99b90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fa709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_streamer = DataStreamer(REDDIT)\n",
    "sample_stream = data_streamer.stream(\n",
    "    ['CarsAustralia'],\n",
    "    ['recommended cheap used 4x4 ute for recreational use'],\n",
    "    progress_bar=True,\n",
    "    limit=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de567b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b9f726488e4f56bfec0ae0394f75c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "streaming subreddit search results...:   0%|          | 0/1 [00:00<?, ?queries/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716230a26287468884003fcd0ac9a463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fetching submissions...:   0%|          | 0/20 [00:00<?, ?posts/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subdata, comdata = [], []\n",
    "for record, data in sample_stream:\n",
    "    if record == \"submission\":\n",
    "        subdata.append(data)\n",
    "    else:\n",
    "        comdata.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3fc5ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission records: 20\n",
      "Comment records: 650\n"
     ]
    }
   ],
   "source": [
    "print(f'Submission records: {len(subdata)}')\n",
    "print(f'Comment records: {len(comdata)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61351cc3",
   "metadata": {},
   "source": [
    "### Transforming the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer():\n",
    "    def transform(self, table:pa.Table):\n",
    "        \"\"\"\n",
    "        Perform cleaning and preprocessing on current batch's PyArrow Table before writing to disk.\n",
    "        Args:\n",
    "            table: PyArrow table containing either submission or comment data.\n",
    "        Returns:\n",
    "            Wrangled PyArrow table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            out_table = self.remove_short_text(table)\n",
    "            out_table = self.remove_matching_text(out_table)\n",
    "        except Exception as e:\n",
    "            print(f'Exception ignored: {e}. Returning untampered PyArrow table.')\n",
    "            return table\n",
    "        else:\n",
    "            return out_table\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Alias for self.transform. Takes a PyArrow table and returns a transformed copy.\"\"\"\n",
    "        return self.transform(*args **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_short_text(table:pa.Table, min_title_length:int=20, min_selftext_length:int=100, min_body_length:int=20):\n",
    "        \"\"\"\n",
    "        Filter out short text from submission and comment dataset to retain only contextually rich textual data.\n",
    "        Args:\n",
    "            table: PyArrow Table for either submission or comment data\n",
    "            min_title_length: Int >= 0 for minimum title text length (UTF-8)\n",
    "            min_selftext_length: Int >= 0 for minimum post selftext length (UTF-8)\n",
    "            min_body_length: Int >= 0 for minimum comment body length (UTF-8)\n",
    "        Returns:\n",
    "            PyArrow table with short text records removed.\n",
    "        \"\"\"\n",
    "        # Build the masking expression based on current dataset\n",
    "        # Match all records that pass minimum character count for relevant columns\n",
    "        data_source = table.schema.metadata[b'data_source']\n",
    "        if data_source == b'submission':\n",
    "            mask_expr = (\n",
    "                (pc.utf8_length(pc.field('title')) < min_title_length) | \n",
    "                (pc.utf8_length(pc.field('selftext')) < min_selftext_length)\n",
    "                )\n",
    "        elif data_source == b'comment':\n",
    "            mask_expr = (pc.utf8_length(pc.field('body')) < min_body_length)\n",
    "        # Return the table without short text\n",
    "        return table.filter(~mask_expr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_matching_text(\n",
    "        table, \n",
    "        regex_patterns:Union[Tuple[str],List[str]]=(r\"\\[deleted\\]\",r\"\\[removed\\]\"), \n",
    "        match_logic:Literal['union','intersect']='union'\n",
    "        ):\n",
    "        \"\"\"\n",
    "        FIlter out matching text. By default, deleted and removed text are filtered out from the submission title,\n",
    "        submission selftext, and comment body.\n",
    "        Args:\n",
    "            table: PyArrow Table for either submission or comment data.\n",
    "            regex_patterns: Tuple or list of regex patterns for matching.\n",
    "            match_logic: Union if text can match either of the patterns (OR), Intersect if text must match all (AND).\n",
    "        Returns:\n",
    "            PyArrow table with matching text records removed.\n",
    "        \"\"\"\n",
    "        # Build the regex pattern; Union (OR) by default\n",
    "        regexp = r\"&\".join([pat for pat in regex_patterns]) if match_logic == 'intersect' else r\"|\".join([pat for pat in regex_patterns])\n",
    "        # Build the masking expression based on current dataset\n",
    "        # Match all records that satisfy the regex expression\n",
    "        data_source = table.schema.metadata[b'data_source']\n",
    "        if data_source == b'submission':\n",
    "            mask_expr = (\n",
    "                pc.match_substring_regex(pc.field(\"title\"), regexp)\n",
    "                | pc.match_substring_regex(pc.field(\"selftext\"), regexp)\n",
    "            )\n",
    "        elif data_source == b'comment':\n",
    "            mask_expr = (\n",
    "                pc.match_substring_regex(pc.field(\"body\"), regexp)\n",
    "            )\n",
    "        # Return the table without matching text\n",
    "        return table.filter(~mask_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f9584",
   "metadata": {},
   "source": [
    "#### Testing the DataTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1afe3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer = DataTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "440ede65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-defined schemas and add metadata\n",
    "with open(os.path.join('..','src','usedcaranalytics','config','schemas.json'),'r') as schema_file:\n",
    "    schema_dict = json.load(schema_file)\n",
    "    sub_schema = pa.schema(\n",
    "        [(col, eval(dtype)) for col, dtype in schema_dict['submission'].items()], \n",
    "        metadata={'data_source':'submission'}\n",
    "        )\n",
    "    com_schema = pa.schema(\n",
    "        [(col, eval(dtype)) for col, dtype in schema_dict['comment'].items()], \n",
    "        metadata={'data_source':'comment'}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "649ae14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the submission and comment dataframe to pyarrow table\n",
    "submission_table = pa.Table.from_pandas(submission_df, schema=sub_schema)\n",
    "comment_table = pa.Table.from_pandas(comment_df,schema=com_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e494bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation: 10 submission records.\n",
      "After transformation: 7 submission records.\n"
     ]
    }
   ],
   "source": [
    "print(f'Before transformation: {len(submission_table)} submission records.')\n",
    "print(f'After transformation: {len(data_transformer.remove_matching_text(submission_table))} submission records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "454d705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation: 556 comment records.\n",
      "After transformation: 529 comment records.\n"
     ]
    }
   ],
   "source": [
    "print(f'Before transformation: {len(comment_table)} comment records.')\n",
    "print(f'After transformation: {len(data_transformer.remove_matching_text(comment_table))} comment records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbac01",
   "metadata": {},
   "source": [
    "## Storing the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1709b1",
   "metadata": {},
   "source": [
    "#### Data Storage\n",
    "\n",
    "Data will be stored as Pyarrow tables since Parquet files have higher compression rates resulting in smaller memory footprint, which is beneficial for larger datasets.\n",
    "\n",
    "__Data Storage Logic:__\n",
    "1. Store the file paths for submission data and comment data Parquet files\n",
    "2. Define the schema for the parquet files to preserve data type on export, marginally improve write performance, and avoid silent errors\n",
    "3. Initialize the data generator with the list of search pairs\n",
    "4. Stream data from the generator and store each record in a dictionary buffer. When buffer size reaches target byte size, convert to Pyarrow Table and write to dataset directory as Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f4941df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetDataLoader:\n",
    "    def __init__(self, config:Tuple[NamedTuple], target_MB:Union[int,float]=32.0, transformer:DataTransformer=None):\n",
    "        self.config = config\n",
    "        self._transformer = transformer\n",
    "        self.set_target_mb(target_MB)\n",
    "        self._configure_loader()\n",
    "    \n",
    "    def _configure_loader(self):\n",
    "        \"\"\"Abstracts away the configuration of schemas, buffers, root paths, byte counters.\"\"\"\n",
    "        # Unpack schema from ParquetConfig namedtuples\n",
    "        self._schemas = {ntuple.record_type : ntuple.schema for ntuple in self.config}\n",
    "        # Store the dataset directory root paths per record type\n",
    "        self._dataset_paths = {ntuple.record_type : ntuple.dataset_path for ntuple in self.config}\n",
    "        # Set-up buffers per record type\n",
    "        self._buffers = {\n",
    "            record_type : {\n",
    "                col : [] for col in self._schemas[record_type].names\n",
    "                } \n",
    "            for record_type in self._schemas\n",
    "            }\n",
    "        # Initialize byte counter per record type\n",
    "        self._buffer_sizes = {record_type : 0 for record_type in self._buffers}\n",
    "        # Batch counter for filename\n",
    "        self._batch_counters = {record_type : 0 for record_type in self._buffers}\n",
    "        return self\n",
    "    \n",
    "    def set_target_mb(self, target_MB:Union[int,float]):\n",
    "        \"\"\"\n",
    "        Setter for target_MB. Updates target_MB and TARGET_BYTES attributes. Allows for updating\n",
    "        buffer size targets post-initialization.\n",
    "        \"\"\"\n",
    "        self.target_MB = target_MB\n",
    "        self._target_bytes = int(target_MB * 2 ** 20)\n",
    "        return self\n",
    "    \n",
    "    def load(self, data_stream:Generator):\n",
    "        \"\"\"\n",
    "        Streams data (\"record type\", record_dict) from an input generator, stores the data into a \n",
    "        dictionary buffer, and writes to disk when a target byte size or when the function call has \n",
    "        finished.\n",
    "        \"\"\" \n",
    "        # Stream the data, append to buffer, track buffer size, and when target buffer size\n",
    "        # is met, write the record batch to disk and flush the buffer\n",
    "        for record_type, record in data_stream:\n",
    "            buffer = self._buffers[record_type]\n",
    "                \n",
    "            for col in buffer:\n",
    "                buffer[col].append(record.get(col))\n",
    "            \n",
    "            # Update byte count with current record bytes\n",
    "            record_bytes = len(json.dumps(record, separators=(\",\", \":\")).encode(\"utf-8\"))\n",
    "            self._buffer_sizes[record_type] += record_bytes\n",
    "            \n",
    "            # Export parquet files to dataset directory and flush buffers and byte counters            \n",
    "            if self._buffer_sizes[record_type] >= self._target_bytes:\n",
    "                self._batch_counters[record_type] += 1\n",
    "                self._write(record_type)\n",
    "                buffer, self._buffer_sizes[record_type] = self._flush(buffer)\n",
    "\n",
    "        # Final write for remaining data in both buffers after streaming data\n",
    "        # Only write if there are remaining records to avoid null records in Parquet file\n",
    "        for record_type, buffer in self._buffers.items():\n",
    "            if all(container for container in buffer.values()):\n",
    "                self._write(record_type, buffer)\n",
    "                buffer, self._buffer_sizes[record_type] = self._flush(buffer)\n",
    "                \n",
    "    def _write(self, record_type:str, buffer:Dict[str,List]):\n",
    "        \"\"\"Convert buffer to Pyarrow container, write to Parquet, then flush buffer and byte count.\"\"\"\n",
    "        schema = self._schemas[record_type]\n",
    "        # File name; Ex. SUBMISSION-0001-20250626-201522\n",
    "        fname = (\n",
    "            f'{record_type.upper()}-'\n",
    "            f'{self._batch_counters[record_type]:04d}-'\n",
    "            f'{dt.datetime.now():%Y%m%d-%H%M%S}.parquet'\n",
    "            )\n",
    "        # Relative path\n",
    "        fpath = os.path.join(self._dataset_paths[record_type],fname)\n",
    "        # Convert current buffer to Pyarrow Table and write Parquet files to dataset directory\n",
    "        pa_table = pa.Table.from_pydict(buffer, schema=schema)\n",
    "        # Transform batched data before writing if transformed was passed to DataLoader constructor\n",
    "        if self._transformer:\n",
    "            pa_table = self._transformer.transform(pa_table)\n",
    "        # GZIP for higher compression and long term static storage\n",
    "        pq.write_table(pa_table, where=fpath, compression='GZIP')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _flush(buffer:Dict[str,List]):\n",
    "        \"\"\"Returns a tuple of empty buffer and byte count, in order, for an input buffer.\"\"\"\n",
    "        # Reconstruct buffer and return with empty values\n",
    "        # Also return 0 for assignment to byte count\n",
    "        return {col : [] for col in buffer}, 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        param_dict = {'config':self.config, 'target_MB':self.target_MB}\n",
    "        return f'ParquetDataLoader({\", \".join([f'{k}={v}' for k, v in param_dict.items()])})'\n",
    "    \n",
    "    def __get__(self):\n",
    "        return {\n",
    "            'CONFIG' : self.config,\n",
    "            'target_MB' : self.target_MB,\n",
    "            '_TARGET_BYTES' : self._target_bytes,\n",
    "            '_SCHEMAS' : self._schemas,\n",
    "            '_ROOT_PATHS' : self._dataset_paths,\n",
    "            '_buffers' : self._buffers,\n",
    "            '_buffer_sizes' : self._buffer_sizes,\n",
    "            '_batch_counters' : self._batch_counters\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9ab68",
   "metadata": {},
   "source": [
    "#### Abstracting away the DataLoader configuration step\n",
    "\n",
    "The DataLoader class is intended to be configured with a tuple of ParquetConfig namedtuples. The namedtuples contain named values for: \n",
    "\n",
    "    1) Record type: submission, comments, \n",
    "    2) Dataset path: UsedCarAnalytics/data/processed,\n",
    "    3) Schema: pyarrow.Schema object for submission/comment\n",
    "\n",
    "These utility functions are meant to generate the schemas and dataset directories to generate the ParquetConfig namedtuples used for configuring the ParquetDataLoader. \n",
    "\n",
    "The config variables essentially define the datatypes for submission and comment data when building the PyArrrow tables and exporting to Parquet, and also define the dataset directory where these files will be exported to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a4efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize namedtuple for Parquet Config for ease of access and immutability\n",
    "ParquetConfig = namedtuple('ParquetConfig',['record_type','dataset_path','schema'])\n",
    "\n",
    "def load_schema(schema_path:str, record_type:str) -> pa.Schema:\n",
    "    \"\"\"\n",
    "    Returns a pyarrow schema parsed from a JSON file containing the schema.\n",
    "    Args:\n",
    "        - Path to JSON schema. Parent object must be either 'submission' or 'comment'\n",
    "        and child object must be a dictionary containing column name keys and pyarrow datatype values\n",
    "        Ex. 'submission' : {'submission_id' : 'pa.string()'}\n",
    "    Returns:\n",
    "        - PyArrow Schema object\n",
    "    \"\"\"\n",
    "    with open(schema_path, 'r') as f:\n",
    "        schemas = json.load(f)\n",
    "        schemas = {key.lower() : value for key, value in schemas.items()}\n",
    "        return pa.schema(\n",
    "            [(col, eval(datatype)) for col, datatype in schemas[record_type].items()], \n",
    "            metadata={'data_source':record_type}\n",
    "            )\n",
    "\n",
    "def get_submission_schema(schema_path:Optional[str]=None) -> pa.Schema:\n",
    "    \"\"\"\n",
    "    Returns a boilerplate pyarrow schema for submission data. An optional schema path\n",
    "    can be provided to return a predefined schema.\n",
    "    Args:\n",
    "        - [Optional] path to JSON schema. Parent object must be either 'submission' or 'comment'\n",
    "        and child object must be a dictionary containing column name keys and pyarrow datatype values\n",
    "        Ex. 'submission' : {'submission_id' : 'pa.string()'}\n",
    "    Returns:\n",
    "        - PyArrow Schema object\n",
    "    \"\"\"\n",
    "    if schema_path:\n",
    "        return load_schema(schema_path, 'submission')\n",
    "    return pa.schema(\n",
    "        [\n",
    "            (\"submission_id\", pa.string()),\n",
    "            (\"title\", pa.string()),\n",
    "            (\"selftext\", pa.string()),\n",
    "            (\"score\", pa.int64()),\n",
    "            (\"upvote_ratio\", pa.float64()),\n",
    "            (\"timestamp\", pa.timestamp(\"s\")),\n",
    "            (\"subreddit\", pa.string()),\n",
    "            (\"num_comments\", pa.int64())\n",
    "        ],\n",
    "        metadata={'data_source':'submission'}\n",
    "        )\n",
    "    \n",
    "def get_comment_schema(schema_path:Optional[str]=None) -> pa.Schema:\n",
    "    \"\"\"\n",
    "    Returns a boilerplate pyarrow schema for submission data. An optional schema path\n",
    "    can be provided to return a predefined schema.\n",
    "    Args:\n",
    "        - [Optional] path to JSON schema. Parent object must be either 'submission' or 'comment'\n",
    "        and child object must be a dictionary containing column name keys and pyarrow datatype values\n",
    "        Ex. 'submission' : {'submission_id' : 'pa.string()'}\n",
    "    Returns:\n",
    "        - PyArrow Schema object\n",
    "    \"\"\"\n",
    "    if schema_path:\n",
    "        return load_schema(schema_path, 'comment')\n",
    "    return pa.schema(\n",
    "        [\n",
    "            (\"comment_id\", pa.string()),\n",
    "            (\"body\", pa.string()),\n",
    "            (\"score\", pa.int64()),\n",
    "            (\"timestamp\", pa.timestamp(\"s\")),\n",
    "            (\"subreddit\", pa.string()),\n",
    "            (\"parent_submission_id\", pa.string())\n",
    "        ],\n",
    "        metadata={'data_source':'comment'}\n",
    "        )\n",
    "\n",
    "def get_parquet_configs(\n",
    "    root:str, \n",
    "    subdir:str=\"data/processed\", \n",
    "    dataset_dirs:Union[Tuple[str],List[str]]=('submission-dataset','comment-dataset'),\n",
    "    **schema_kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Utility function to generate tuple of ParquetConfig namedtuples for submission and comment\n",
    "    datasets respectively.\n",
    "    Args:\n",
    "        - Root path.\n",
    "        - Data subdirectory path (default = data/processed).\n",
    "        - Dataset directory names in order: 1) submission dataset, 2) comment dataset \n",
    "        (default = ('submission-dataset','comment-dataset')).\n",
    "    Returns:\n",
    "        - Tuple of namedtuples (sub_cfg, com_cfg) containing record_type, dataset_path, and\n",
    "        schema attribtues\n",
    "    \"\"\"\n",
    "    # Define the dataset directory paths for submission and content data\n",
    "    sub_path, com_path = tuple(\n",
    "        os.path.join(root, subdir, dataset_dir) \n",
    "        for dataset_dir in dataset_dirs\n",
    "    )\n",
    "    \n",
    "    # Get the submission and content data Arrow & Parquet schemas\n",
    "    sub_schema = get_submission_schema(**schema_kwargs)\n",
    "    com_schema = get_comment_schema(**schema_kwargs)\n",
    "\n",
    "    # Build the ParquetConfig files and return as tuple\n",
    "    return tuple(\n",
    "        ParquetConfig(record_type, path, schema) \n",
    "        for record_type, path, schema \n",
    "        in zip(\n",
    "            ('submission','comment'),\n",
    "            (sub_path, com_path),\n",
    "            (sub_schema, com_schema)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904ce7c",
   "metadata": {},
   "source": [
    "## Building the ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c26a17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc45efee5f147ca9ea19e8b21972f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "streaming subreddit search results...:   0%|          | 0/4 [00:00<?, ?queries/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b635d792382e47c49c5d60cbc328e458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fetching submissions...:   0%|          | 0/10 [00:00<?, ?posts/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03f3334cc03492ebaa1b6838ab0f078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fetching submissions...:   0%|          | 0/10 [00:00<?, ?posts/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804569facf6d487f94f9a12aacd26f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fetching submissions...:   0%|          | 0/10 [00:00<?, ?posts/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cba51cd82bc4e73a33c20402bd93f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fetching submissions...:   0%|          | 0/10 [00:00<?, ?posts/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parse text files containing search queries and subreddit names and test only on subset\n",
    "search_queries, subreddits = get_search_pairs('..')\n",
    "search_queries, subreddits = search_queries[:2], subreddits[:2]\n",
    "\n",
    "# Generate config files for loader\n",
    "parquet_configs = get_parquet_configs(\n",
    "    root='..', \n",
    "    schema_path=os.path.join('..','src','usedcaranalytics','config','schemas.json')\n",
    "    )\n",
    "\n",
    "# Initialize the streamer and loader classes, transformer will be initialized by the loader.\n",
    "data_streamer = DataStreamer(REDDIT)\n",
    "data_transformer = DataTransformer()\n",
    "data_loader = ParquetDataLoader(\n",
    "    config=parquet_configs, \n",
    "    transformer=data_transformer, \n",
    "    target_MB=128\n",
    "    )\n",
    "\n",
    "# Streamer will stream submission and search records to loader, records are stored to in-memory\n",
    "# buffers. When buffer size hits target MB, buffer is converted to PyArrow table and transformed\n",
    "# by transformer via field expressions. Transformed table is written to disk by loader.\n",
    "stream = data_streamer.stream(\n",
    "    subreddits=subreddits, \n",
    "    queries=search_queries, \n",
    "    limit=10,\n",
    "    progress_bar=True\n",
    "    )\n",
    "data_loader.load(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b201ec",
   "metadata": {},
   "source": [
    "#### Load Parquet files from submission and comment datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b21d0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset_dir = Path(parquet_configs[0].dataset_path)\n",
    "com_dataset_dir = Path(parquet_configs[1].dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a60c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed/submission-dataset/SUBMISSION-0000-20250629-164513.parquet\n",
      "../data/processed/comment-dataset/COMMENT-0000-20250629-164513.parquet\n"
     ]
    }
   ],
   "source": [
    "# Get the latest files\n",
    "sub_pqt_file = max(list(sub_dataset_dir.glob(\"*.parquet\")), key=lambda file: file.stat().st_mtime)\n",
    "com_pqt_file = max(list(com_dataset_dir.glob(\"*.parquet\")), key=lambda file: file.stat().st_mtime)\n",
    "print(sub_pqt_file)\n",
    "print(com_pqt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c9125fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read exported parquet files as pandas dataframe\n",
    "etl_submission_data = pd.read_parquet(sub_pqt_file)\n",
    "etl_comment_data = pd.read_parquet(com_pqt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43b04c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1d0xqjx</td>\n",
       "      <td>Most reliable used car, under $25k, less than ...</td>\n",
       "      <td>I’m thinking it’s likely a Toyota or Honda, bu...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2024-05-26 09:33:21</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1jx4l2a</td>\n",
       "      <td>Used SUV under $20k (need a reliable car ASAP)</td>\n",
       "      <td>_Reposting this because I didn’t get any comme...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2025-04-12 00:12:52</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18fr75g</td>\n",
       "      <td>What would be the cheap-to-run-and-maintain ca...</td>\n",
       "      <td>Hello, I am looking for a car with good fuel e...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2023-12-11 10:04:03</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1gjb97c</td>\n",
       "      <td>Just moved to Australia, looking for a used sm...</td>\n",
       "      <td>We have two small kids, but have access to a 4...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2024-11-04 10:36:10</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1dnb05z</td>\n",
       "      <td>Reliable used car for $10-15k</td>\n",
       "      <td>I’m in the market for a used car with a budget...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2024-06-24 11:16:27</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id                                              title  \\\n",
       "0       1d0xqjx  Most reliable used car, under $25k, less than ...   \n",
       "1       1jx4l2a     Used SUV under $20k (need a reliable car ASAP)   \n",
       "2       18fr75g  What would be the cheap-to-run-and-maintain ca...   \n",
       "3       1gjb97c  Just moved to Australia, looking for a used sm...   \n",
       "4       1dnb05z                      Reliable used car for $10-15k   \n",
       "\n",
       "                                            selftext  score  upvote_ratio  \\\n",
       "0  I’m thinking it’s likely a Toyota or Honda, bu...     27          0.92   \n",
       "1  _Reposting this because I didn’t get any comme...      2          1.00   \n",
       "2  Hello, I am looking for a car with good fuel e...     30          0.63   \n",
       "3  We have two small kids, but have access to a 4...      3          0.71   \n",
       "4  I’m in the market for a used car with a budget...      5          1.00   \n",
       "\n",
       "            timestamp        subreddit  num_comments  \n",
       "0 2024-05-26 09:33:21  r/CarsAustralia           120  \n",
       "1 2025-04-12 00:12:52  r/CarsAustralia            20  \n",
       "2 2023-12-11 10:04:03  r/CarsAustralia            74  \n",
       "3 2024-11-04 10:36:10  r/CarsAustralia             9  \n",
       "4 2024-06-24 11:16:27  r/CarsAustralia            11  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(etl_submission_data.shape)\n",
    "etl_submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bc8ede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1049, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>parent_submission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l5q3ib3</td>\n",
       "      <td>Mazda 3, Corolla, i30 would be my pick. I had ...</td>\n",
       "      <td>26</td>\n",
       "      <td>2024-05-26 09:40:02</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>1d0xqjx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l5q6wxs</td>\n",
       "      <td>Almost anything Japanese</td>\n",
       "      <td>41</td>\n",
       "      <td>2024-05-26 10:23:50</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>1d0xqjx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l5q74n2</td>\n",
       "      <td>Toyota Camry - boring as hell but super reliab...</td>\n",
       "      <td>27</td>\n",
       "      <td>2024-05-26 10:26:31</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>1d0xqjx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l5qc7ch</td>\n",
       "      <td>Loved my Subaru Outback</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-05-26 11:25:21</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>1d0xqjx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l5qbgn0</td>\n",
       "      <td>How about honda civic guys?</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-05-26 11:17:21</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>1d0xqjx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id                                               body  score  \\\n",
       "0    l5q3ib3  Mazda 3, Corolla, i30 would be my pick. I had ...     26   \n",
       "1    l5q6wxs                           Almost anything Japanese     41   \n",
       "2    l5q74n2  Toyota Camry - boring as hell but super reliab...     27   \n",
       "3    l5qc7ch                            Loved my Subaru Outback      5   \n",
       "4    l5qbgn0                        How about honda civic guys?      5   \n",
       "\n",
       "            timestamp        subreddit parent_submission_id  \n",
       "0 2024-05-26 09:40:02  r/CarsAustralia              1d0xqjx  \n",
       "1 2024-05-26 10:23:50  r/CarsAustralia              1d0xqjx  \n",
       "2 2024-05-26 10:26:31  r/CarsAustralia              1d0xqjx  \n",
       "3 2024-05-26 11:25:21  r/CarsAustralia              1d0xqjx  \n",
       "4 2024-05-26 11:17:21  r/CarsAustralia              1d0xqjx  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(etl_comment_data.shape)\n",
    "etl_comment_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
