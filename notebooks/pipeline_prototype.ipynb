{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d1e5ad",
   "metadata": {},
   "source": [
    "# Data Pipeline Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a7928",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044418d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas praw prawcore python-dotenv pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw, prawcore, time, os, sys, functools, random\n",
    "import datetime as dt\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "from collections.abc import Callable, Iterator\n",
    "from itertools import product\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file for access keys\n",
    "load_dotenv(os.path.join('..', 'config', '.env'))\n",
    "\n",
    "# Import config.py to access environment variables\n",
    "sys.path.append('../config')\n",
    "from config import PRAW_ID, PRAW_SECRET, PRAW_USER_AGENT, PRAW_USERNAME, PRAW_PASSWORD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bb44f",
   "metadata": {},
   "source": [
    "## Setting up access to Reddit API\n",
    "Access keys to Reddit API are stored in a .env file under the config directory of this repository. A template for the .env file is provided in the config directory.\n",
    "\n",
    "The config.py script assigns the environment variables to the `PRAW_ID`, `PRAW_SECRET`, `PRAW_USER_AGENT`, `PRAW_USERNAME`, and `PRAW_PASSWORD` global variables respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRAW \n",
    "reddit = praw.Reddit(\n",
    "    client_id = PRAW_ID,\n",
    "    client_secret = PRAW_SECRET,\n",
    "    username = PRAW_USERNAME,\n",
    "    password = PRAW_PASSWORD,\n",
    "    user_agent = PRAW_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01d930",
   "metadata": {},
   "source": [
    "## Extracting text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88863fd",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "The helper functions were designed to extract relevant data and metadata from Reddit submissions and comments, and package the data into a dict of dicts that can be easily parsed into a Pandas DataFrame object for further analysis.\n",
    "\n",
    "`backoff_on_rate_limit`: This is a decorator factory that builds a custom decorator based on specified backoff parameters (max retries, base delay, cap, jitter). The decorator itself is a wrapper for custom functions that call PRAW methods such as `fetch_submissions` and `fetch_comments`, which call subreddit.search() and submission.comments.replace_more() respectively. The decorator implements exponential backoff with optional full jitter to respect Reddit API rate limits while handling transient failures.\n",
    "\n",
    "__Inputs:__\n",
    "- Integer value for max retries. When attempts exceed this number, an Exception is raised\n",
    "- Float for base delay in seconds (i.e. Delay at first failed attempt)\n",
    "- Float for maximum delay in seconds\n",
    "- Bool on whether to implement full jitter or not\n",
    "\n",
    "__Outputs:__\n",
    "- Decorator to be applied to an PRAW API request wrapper\n",
    "\n",
    "`parse_comments`: This is a utility function that fetches comments from a given post and formats each comment as a dictionary of dictionaries with key as comment id and value as a dictionary of comment content and metadata (e.g. body, timestamp, upvotes).\n",
    "\n",
    "__Inputs:__ \n",
    "- Submission object from PRAW (i.e. Reddit posts)\n",
    "- Integer for .replace_more limit parameter, default=0 (i.e. top/parent comments only)\n",
    "\n",
    "__Output:__\n",
    "- Dict of comments in the format {comment_id : {data_header: data_value}}\n",
    "\n",
    "`parse_search_results`: This is a utility function that fetches submissions (posts) from a given subreddit using a predefined search query (i.e. keywords). Submissions are formatted into a dict of dicts with format {submission id : {data_header : data_value}}. This returns a tuple of submission data and comment data.\n",
    "\n",
    "__Inputs:__ \n",
    "- String of Subreddit name\n",
    "- String of search query\n",
    "- Integer for limit of submissions yielded by PRAW subreddit search\n",
    "\n",
    "__Output:__\n",
    "- Tuple of submission data dict and comment data dict\n",
    "\n",
    "__Read more:__\n",
    "1. [API Rate Limits Explained: Best Practices for 2025](https://orq.ai/blog/api-rate-limit)\n",
    "2. [Exponential Backoff And Jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)\n",
    "3. [Yield Statements vs. Returning Lists in Python](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://community.aws/content/2h01Byx1ytU8357tp2bvcUuJ2j0/yield-statements-vs-returning-lists-in-python%23:~:text%3DYield%253A%2520Ideal%2520for%2520large%2520data,potentially%2520leading%2520to%2520memory%2520errors.&ved=2ahUKEwjzvJvd74uOAxVkQ6QEHVAVMHcQFnoECBIQAw&usg=AOvVaw3hMoJHnPwBIQOdBmB_NiBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "8a6d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_on_rate_limit(max_retries:int=5, \n",
    "                        base_delay:float=1.0, \n",
    "                        cap_delay:float=60.0, \n",
    "                        jitter:bool=True) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator factory that applies exponential backoff (with optional jitter)\n",
    "    when Reddit API rate limits (HTTP 429) or server errors occur.\n",
    "    Stops after max_retries and re-raises the exception.\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Start with base delay, then exponentially scale by attempt\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except prawcore.exceptions.ResponseException as e:\n",
    "                    if attempt > max_retries:\n",
    "                        raise Exception(\"Max retries exceeded with Reddit API.\")\n",
    "                    delay = min(cap_delay, base_delay * 2 ** attempt)\n",
    "                    if jitter:\n",
    "                        delay = random.uniform(0, delay)\n",
    "                    print(f\"[WARNING] {e.__class__.__name__} on attempt {attempt+1}, retrying after {delay:.2f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    attempt += 1\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_submissions(subreddit:object, query:str, limit:int=100, **kwargs) -> Iterator:\n",
    "    \"\"\"Modify the subreddit search from PRAw to ensure adherence to safe request limits.\"\"\"\n",
    "    return subreddit.search(**kwargs, query=query, limit=limit)\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_comments(submission:object, limit:int=0) -> list:\n",
    "    \"\"\"Modify the comment fetch from PRAW to ensure adherence to safe request limits.\"\"\"\n",
    "    submission.comments.replace_more(limit=limit)\n",
    "    return submission.comments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_comments(submission:object, limit:int=0):\n",
    "    \"\"\"\n",
    "    Fetches comments from a Submission objects then parses each comment into a dictionary record.\n",
    "    Each entry is streamed for efficient memory footprint when handling larger CommentForests.\n",
    "    \"\"\"\n",
    "    # Dict of dicts with format {comment_id : comment_info_dict}\n",
    "    comment_data: Dict[str, Dict[str, Any]] = {}\n",
    "    # Update comments dict with info dict \n",
    "    for comment in fetch_comments(submission, limit=limit):\n",
    "        record = {\n",
    "            'comment_id':comment.id,\n",
    "            'body':comment.body,\n",
    "            'score':comment.score,\n",
    "            'timestamp':comment.created_utc,\n",
    "            'subreddit':comment.subreddit_name_prefixed,\n",
    "            'parent_submission_id':submission.id\n",
    "        }\n",
    "        # Stream\n",
    "        yield \"comment\", record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_submissions_and_comments(subreddit_name:str, query:str, limit:int=50, **search_kwargs):\n",
    "    \"\"\"\n",
    "    Fetches submissions, parses each submission into a dictionary record, and calls the stream_comments\n",
    "    function on each submission. Submission data and comment data are streamed for efficient memory \n",
    "    footprint when handling larger datasets. \n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Fetch submissions, and for every submission, fetch the comments\n",
    "    for submission in fetch_submissions(**search_kwargs, subreddit=sub, query=query, limit=limit):\n",
    "        # Update submissions dict with info dict from submission\n",
    "        record = {\n",
    "            'submission_id':submission.id,\n",
    "            'title':submission.title,\n",
    "            'selftext':submission.selftext,\n",
    "            'score':submission.score,\n",
    "            'upvote_ratio':submission.upvote_ratio,\n",
    "            'timestamp':submission.created_utc,\n",
    "            'subreddit':submission.subreddit_name_prefixed,\n",
    "            'num_comments':submission.num_comments\n",
    "            }\n",
    "        # Stream comment data from current submission\n",
    "        yield from stream_comments(submission)\n",
    "        # Stream submission data\n",
    "        yield \"submission\", record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b0f1e",
   "metadata": {},
   "source": [
    "### Provide an initial list of search queries and Subreddits\n",
    "\n",
    "To scrape the relevant text data from Reddit, I created a small list of queries covering diverse yet relevant topics to buying affordable used vehicles. The queries involved location-specific, model-specific, and thematic keywords to ensure that the search covers as much ground as possible. Chosen subreddits have > 1e5 subscribers to ensure that search queries will yield a significant amount of results per API request.\n",
    "\n",
    "With a 10x10 query and subreddit array, I expect at least an initial 100 requests for the subreddit search yielding 100x50 submissions at most.\n",
    "\n",
    "Fetching the comments involves significantly more requests as each submission requires 1 request to yield the CommentForest. Fetching the comments will require at least 10,000 requests.\n",
    "\n",
    "__Expected Minimum API Requests__\n",
    "|Search Requests|Comment Fetch Requests|Total Requests|\n",
    "|:----------|:----------|:----------|\n",
    "|100      |5,000  |5,100|\n",
    "\n",
    "As such, a single batch job covering all query-subreddit combinations will yield at least 10,100 API requests in a single go, which wildly exceeds the Reddit API fair use policy (i.e. Cap requests to 100/min averaged over 10-minute sliding window). To address this issue, batched processing will be implemented to ensure average requests is under safe rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da66db6",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "`parse_txt_file`: Parses text files containing data separated by newlines. Returns a list. Used for containerizing search_queries and subreddit strings into separate text files that can be easily mutated without modifying source code.\n",
    "\n",
    "__Input:__\n",
    "- String for the path of text file, with each item separated by a newline\n",
    "\n",
    "__Output:__\n",
    "- List (e.g. search queries, subreddit names)\n",
    "\n",
    "`aggregate_search_results`: This function is a wrapper for the `parse_search_results` call and calls the inner function for each subreddit-query pair formed from the input list arguments. Requests are tracked at every iteration and compared against maximum requests per minute. If expected total requests go beyond rate limit, program execution is paused for at least a minute to ensure that requests are within safe rate limits. The number of submissions requested are also randomized per search pair to reduce predictability of scraping pattern.\n",
    "\n",
    "__Inputs:__ \n",
    "- List of subreddit name strings\n",
    "- List of search query strings\n",
    "- Int of maximum requests per minute, also determines upper bound of search result limit\n",
    "- Int of minimum requests, which is the floor of search result limit\n",
    "- List of float values denoting delay in seconds for long delay (interval between search pairs); minimum of 60s\n",
    "\n",
    "__Output:__\n",
    "- Tuple of aggregated submissions dict and comments dict\n",
    "\n",
    "__Read more:__\n",
    "1. [Rate Limiter - Sliding Window Counter](https://medium.com/@avocadi/rate-limiter-sliding-window-counter-7ec08dbe21d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "48b8744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_file(file_path:str):\n",
    "    \"\"\"\n",
    "    Utility function for parsing a multi-line text file where each item is separated\n",
    "    by a newline.\n",
    "    Input: String for file path\n",
    "    Output: List\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Ignore comments and empty lines\n",
    "        results = [line.rstrip(\"\\n\") for line in f if not (line.startswith('#') or line.startswith(\"\\n\"))]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9cf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_aggregate_results(subreddits:List[str], \n",
    "                             queries:List[str],\n",
    "                             max_requests:int=100, \n",
    "                             min_requests:int=50,\n",
    "                             jitter:List[float] = [1.0,10.0],\n",
    "                             **search_kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for streaming functions. Takes a list of subreddits and queries, then calls the \n",
    "    stream_search_results  function for each combination of subreddit and query. Jitter is implemented \n",
    "    to introduce randomness in number of API requests with a short backoff in each iteration to ensure\n",
    "    adherence to Reddit API rate limits.\n",
    "    \"\"\"\n",
    "    assert isinstance(subreddits, list), \"Argument 'subreddits' expects a list of subreddit names.\"\n",
    "    assert isinstance(queries, list), \"Argument 'queries' expects a list of search queries names.\"\n",
    "    \n",
    "    # API request counter for triggering execution cooldown\n",
    "    Trace = namedtuple('Trace', ['timestamp','total_requests'])\n",
    "    trace_requests = []\n",
    "    total_requests = 0\n",
    "    \n",
    "    # Parse submission and comment data with jittered API calls\n",
    "    for subreddit, query in product(subreddits, queries):\n",
    "        # Random number of requests per iteration to reduce predictability\n",
    "        submission_limit = int(random.uniform(min_requests, max_requests))\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = dt.datetime.now()\n",
    "        \n",
    "        # Stream data \n",
    "        stream_submissions_and_comments(**search_kwargs, subreddit_name=subreddit, \n",
    "                                        query=query, limit=submission_limit)\n",
    "        \n",
    "        # Record end time and compare against window (start + 1 minute)\n",
    "        end_time = dt.datetime.now()\n",
    "        \n",
    "        # Trigger delay to ensure average requests are under 100 per minute\n",
    "        window = start_time + dt.timedelta(minutes=1)\n",
    "        delay = abs(window - end_time).seconds + random.uniform(*jitter)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Update total requests and request history\n",
    "        total_requests += submission_limit\n",
    "        trace_requests.append(Trace(dt.datetime.now(), total_requests))\n",
    "    \n",
    "    print(f'Finished writing data to disk.\\nTotal requests made: {sum(trace_requests)};\\nTrace: {\"\\n\".join(trace_requests)}.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay_next_execution(period:float=60.0, jitter:List[float]=[0.1,5.0]):\n",
    "    \"\"\"Decorator factory for delaying next execution by at least the residual time from\n",
    "    the end of function execution and the remaining time window.\"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            \"\"\"\n",
    "            Wrap generator function with a timer that ensures at least the stated period\n",
    "            elapsed from the start of the recently executed function to the evaluation of\n",
    "            the next expression.\n",
    "            \"\"\"\n",
    "            start_time = dt.datetime.now()\n",
    "            yield from func(*args, **kwargs)\n",
    "            end_time = dt.datetime.now()\n",
    "            window = start_time + dt.timedelta(seconds=period)\n",
    "            if window > end_time:\n",
    "                delay = abs(window - end_time).seconds + random.uniform(*jitter)\n",
    "                time.sleep(delay)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "26d118ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 20:03:26\n",
      "Ended at 20:03:26\n",
      "Elapsed 2.26s before next batch.\n",
      "[1, 2, 3, 4, 5]\n",
      "Started at 20:03:28\n",
      "Ended at 20:03:28\n",
      "Elapsed 1.05s before next batch.\n",
      "[1, 2, 3, 4, 5]\n",
      "Started at 20:03:29\n",
      "Ended at 20:03:29\n",
      "Elapsed 0.75s before next batch.\n",
      "[1, 2, 3, 4, 5]\n",
      "Started at 20:03:30\n",
      "Ended at 20:03:30\n",
      "Elapsed 0.63s before next batch.\n",
      "[1, 2, 3, 4, 5]\n",
      "Started at 20:03:31\n",
      "Ended at 20:03:31\n",
      "Elapsed 1.66s before next batch.\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(list(foo()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a5a7aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_window = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "66973d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = (start_window + dt.timedelta(minutes=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fe336a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(window - dt.datetime.now()).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "45db7b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dt.datetime.now() - window).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7e8c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_requests = [500,20,30,65,32]\n",
    "timestamp = [600,650,670,700,710]\n",
    "Trace = namedtuple('Trace',['timestamp','requests'])\n",
    "traces = deque()\n",
    "for ts, r in zip(timestamp, total_requests):\n",
    "    traces.append(Trace(ts, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "731f688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_window = dt.datetime.now()\n",
    "previous_window = current_window - dt.timedelta(minutes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b90fb590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 6, 25, 18, 42, 1, 971572)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "499837e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window1 > current_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "979c1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "window1 = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dbc3241f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 6, 25, 18, 32, 1, 971572)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7de1597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_requests = 0\n",
    "max_requests = 1000\n",
    "while traces and window_requests < max_requests:\n",
    "    window_requests += traces.popleft().requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f45d241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "647"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2851671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_time(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5111de9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_sleep_window(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dba21d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([datetime.datetime(2025, 6, 25, 17, 52, 48, 516068),\n",
       "       datetime.datetime(2025, 6, 25, 17, 53, 13, 343613)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b06d7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse text files containing search queries and subreddit names\n",
    "search_queries = parse_txt_file(\"../src/search_queries.txt\")\n",
    "subreddits = parse_txt_file(\"../src/subreddits.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9073dfd",
   "metadata": {},
   "source": [
    "### Fetching and parsing search results from Reddit used car communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "193078da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Search Pairs-------\n",
      "CarsAustralia - affordable reliable used cars under 15k Australia\n",
      "CarsAustralia - affordable reliable used cars under 10k USA\n",
      "UsedCars - affordable reliable used cars under 15k Australia\n",
      "UsedCars - affordable reliable used cars under 10k USA\n"
     ]
    }
   ],
   "source": [
    "search_queries = search_queries[:2]\n",
    "subreddits = subreddits[:2]\n",
    "print('-------Search Pairs-------')\n",
    "for (subreddit, query) in product(subreddits, search_queries):\n",
    "    print(subreddit,\"-\",query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "dcb454e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooldown triggered: sleeping for 109.40741414375071s to avoid rate limit.\n",
      "Cooldown triggered: sleeping for 71.57586797057071s to avoid rate limit.\n",
      "Cooldown triggered: sleeping for 65.00197525949584s to avoid rate limit.\n",
      "CPU times: user 2.8 s, sys: 314 ms, total: 3.11 s\n",
      "Wall time: 7min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fetch search results and parse to dict of dicts\n",
    "submission_data, comment_data = aggregate_search_results(subreddits=subreddits, queries=search_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528009c",
   "metadata": {},
   "source": [
    "## Storing the scraped data\n",
    "\n",
    "### Formatting to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "cf8279db",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame.from_dict(submission_data, orient='index')\n",
    "comment_df = pd.DataFrame.from_dict(comment_data, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f22123",
   "metadata": {},
   "source": [
    "### Exporting DataFrame to a Parquet file for efficient storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5c641d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_parquet(os.path.join(\"..\",\"data\",\"submission_data.parquet\"), \n",
    "                         engine='pyarrow',\n",
    "                         compression='gzip')\n",
    "\n",
    "comment_df.to_parquet(os.path.join(\"..\",\"data\",\"comment_data.parquet\"),\n",
    "                      engine='pyarrow',\n",
    "                      compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dba01",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
