{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d1e5ad",
   "metadata": {},
   "source": [
    "# Data Pipeline Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a7928",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044418d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas praw prawcore python-dotenv pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a9c3309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw, prawcore, time, os, sys, functools, random\n",
    "import datetime as dt\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any, Union, Tuple\n",
    "from itertools import product\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file for access keys\n",
    "load_dotenv(os.path.join('..', 'config', '.env'))\n",
    "\n",
    "# Import config.py to access environment variables\n",
    "sys.path.append('../config')\n",
    "from config import PRAW_ID, PRAW_SECRET, PRAW_USER_AGENT, PRAW_USERNAME, PRAW_PASSWORD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bb44f",
   "metadata": {},
   "source": [
    "## Setting up access to Reddit API\n",
    "Access keys to Reddit API are stored in a .env file under the config directory of this repository. A template for the .env file is provided in the config directory.\n",
    "\n",
    "The config.py script assigns the environment variables to the `PRAW_ID`, `PRAW_SECRET`, `PRAW_USER_AGENT`, `PRAW_USERNAME`, and `PRAW_PASSWORD` global variables respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRAW \n",
    "reddit = praw.Reddit(\n",
    "    client_id = PRAW_ID,\n",
    "    client_secret = PRAW_SECRET,\n",
    "    username = PRAW_USERNAME,\n",
    "    password = PRAW_PASSWORD,\n",
    "    user_agent = PRAW_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01d930",
   "metadata": {},
   "source": [
    "## Extracting text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88863fd",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "#### Decorator for implementing Exponential Backoff and Full Jitter\n",
    "`backoff_on_rate_limit`: This is a decorator factory that builds a custom decorator based on specified backoff parameters (max retries, base delay, cap, jitter). The decorator itself is a wrapper for custom functions that call PRAW methods such as `fetch_submissions` and `fetch_comments`, which call subreddit.search() and submission.comments.replace_more() respectively. The decorator implements exponential backoff with optional full jitter to respect Reddit API rate limits while handling transient failures.\n",
    "<blockquote>\n",
    "\n",
    "__Inputs:__\n",
    "- Integer value for max retries. When attempts exceed this number, an Exception is raised\n",
    "- Float for base delay in seconds (i.e. Delay at first failed attempt)\n",
    "- Float for maximum delay in seconds\n",
    "- Bool on whether to implement full jitter or not\n",
    "\n",
    "__Outputs:__\n",
    "- Decorator to be applied to an PRAW API request wrapper\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_on_rate_limit(max_retries:int=5, \n",
    "                        base_delay:float=1.0, \n",
    "                        cap_delay:float=60.0, \n",
    "                        jitter:bool=True):\n",
    "    \"\"\"\n",
    "    Decorator factory that applies exponential backoff (with optional jitter)\n",
    "    when Reddit API rate limits (HTTP 429) or server errors occur.\n",
    "    Stops after max_retries and re-raises the exception.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Start with base delay, then exponentially scale by attempt\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except prawcore.exceptions.ResponseException as e:\n",
    "                    if attempt > max_retries:\n",
    "                        raise Exception(\"Max retries exceeded with Reddit API.\")\n",
    "                    delay = min(cap_delay, base_delay * 2 ** attempt)\n",
    "                    if jitter:\n",
    "                        delay = random.uniform(0, delay)\n",
    "                    print(f\"[WARNING] {e.__class__.__name__} on attempt {attempt+1}, retrying after {delay:.2f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    attempt += 1\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5492186",
   "metadata": {},
   "source": [
    "#### Rate Limiter Class\n",
    "\n",
    "The RateLimiter class is initialized at the beginning of the script and is used to track API requests made within a specific sliding window. Requests are throttled when total expected requests go beyond rate limits (Reddit = 100/min) for the current window. Jitter is injected to the wait time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ac723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    \"\"\"Rate Limiter with sliding window implementation.\"\"\"\n",
    "    def __init__(self, max_requests:int=100, period:float=60.0, jitter:Union[List[float],Tuple[float]]=(0.1,5.0)):\n",
    "        self.max_requests = max_requests\n",
    "        self.period = period\n",
    "        self.jitter = jitter\n",
    "        self.trace_requests = deque()\n",
    "        \n",
    "    def wait_for_slot(self, n_request:int=1) -> None:\n",
    "        \"\"\"\n",
    "        Delays execution of subsequent API request or code chunk to ensure maximum\n",
    "        function calls or request adheres to rate limits within a specified window.\n",
    "        \"\"\"\n",
    "        window_end = time.time()\n",
    "        window_start = window_end - self.period\n",
    "        \n",
    "        # Remove older batches when timestamp is out of current window\n",
    "        while self.trace_requests and self.trace_requests[0] < window_start:\n",
    "            self.trace_requests.popleft()\n",
    "        \n",
    "        # Check if additional request can be accommodated given requests made in current window\n",
    "        if len(self.trace_requests) + n_request > self.max_requests:\n",
    "            # Wait time is adjusted by jitter\n",
    "            wait_time = (self.trace_requests[0] + self.period) - window_end + random.uniform(*self.jitter)\n",
    "            time.sleep(max(wait_time, 0))\n",
    "            # Re-run the function and determine if request can be accommodated\n",
    "            return self.wait_for_slot(n_request)\n",
    "        \n",
    "        # Enqueue current request to trace requests\n",
    "        for _ in range(n_request):\n",
    "            self.trace_requests.append(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7280719",
   "metadata": {},
   "source": [
    "### API Call Wrappers\n",
    "\n",
    "The helper functions were designed to extract relevant data and metadata from Reddit submissions and comments, and package the data into a dict of dicts that can be easily parsed into a Pandas DataFrame object for further analysis. The backoff decorator is applied to each API call wrapper to handle transient errors raised by HTTP 429 response (Too Many Requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff_on_rate_limit()\n",
    "def fetch_submissions(subreddit:object, query:str, limit:int=100, **kwargs):\n",
    "    \"\"\"Modify the subreddit search from PRAw to ensure adherence to safe request limits.\"\"\"\n",
    "    return subreddit.search(**kwargs, query=query, limit=limit)\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_comments(submission:object, limit:int=0):\n",
    "    \"\"\"Modify the comment fetch from PRAW to ensure adherence to safe request limits.\"\"\"\n",
    "    # Replace 'more' with specified limit (default = 0 or retain top-level comments only)\n",
    "    submission.comments.replace_more(limit=limit)\n",
    "    for comment in submission.comments:\n",
    "        yield comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2dc7a",
   "metadata": {},
   "source": [
    "#### Data Streaming Functions\n",
    "\n",
    "`stream_comments`: This is a utility function that fetches comments from a given post and formats each comment as a dictionary of dictionaries with key as comment id and value as a dictionary of comment content and metadata (e.g. body, timestamp, upvotes).\n",
    "<blockquote>\n",
    "\n",
    "__Inputs:__ \n",
    "- Submission object from PRAW (i.e. Reddit posts)\n",
    "- Integer for .replace_more limit parameter, default=0 (i.e. top/parent comments only)\n",
    "\n",
    "__Output:__\n",
    "- Dict of comments in the format {comment_id : {data_header: data_value}}\n",
    "</blockquote>\n",
    "\n",
    "`stream_submissions_and_comments`: This is a utility function that fetches submissions (posts) from a given subreddit using a predefined search query (i.e. keywords). Submissions are formatted into a dict of dicts with format {submission id : {data_header : data_value}}. This returns a tuple of submission data and comment data.\n",
    "<blockquote>\n",
    "\n",
    "__Inputs:__ \n",
    "- String of Subreddit name\n",
    "- String of search query\n",
    "- Integer for limit of submissions yielded by PRAW subreddit search\n",
    "\n",
    "__Output:__\n",
    "- Tuple of submission data dict and comment data dict\n",
    "</blockquote>\n",
    "\n",
    "`stream_aggregate_results`: This function is a wrapper for the `stream_submissions_and_comments` generator function and takes a list of subreddit names and search queries to feed the subreddit-query pairs to the wrapped function. A time delay is included between every inner function call to ensure adherence to the 100 requests/minute rate limit.\n",
    "<blockquote>\n",
    "\n",
    "__Inputs:__ \n",
    "- List of subreddit name strings\n",
    "- List of search query strings\n",
    "- Int of maximum requests per minute, also determines upper bound of search result limit\n",
    "- Int of minimum requests, which is the floor of search result limit\n",
    "- Float of seconds denoting the time period for counting the API call limits\n",
    "- List of float values of seconds to randomly add to interval delay \n",
    "\n",
    "__Output:__\n",
    "- Tuple of aggregated submissions dict and comments dict\n",
    "</blockquote>\n",
    "\n",
    "__Read more:__\n",
    "1. [Rate Limiter - Sliding Window Counter](https://medium.com/@avocadi/rate-limiter-sliding-window-counter-7ec08dbe21d6)\n",
    "\n",
    "__Read more:__\n",
    "1. [API Rate Limits Explained: Best Practices for 2025](https://orq.ai/blog/api-rate-limit)\n",
    "2. [Exponential Backoff And Jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)\n",
    "3. [Yield Statements vs. Returning Lists in Python](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://community.aws/content/2h01Byx1ytU8357tp2bvcUuJ2j0/yield-statements-vs-returning-lists-in-python%23:~:text%3DYield%253A%2520Ideal%2520for%2520large%2520data,potentially%2520leading%2520to%2520memory%2520errors.&ved=2ahUKEwjzvJvd74uOAxVkQ6QEHVAVMHcQFnoECBIQAw&usg=AOvVaw3hMoJHnPwBIQOdBmB_NiBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_comments(submission:object, limit:int=0):\n",
    "    \"\"\"\n",
    "    Fetches comments from a Submission objects then parses each comment into a dictionary record.\n",
    "    Each entry is streamed for efficient memory footprint when handling larger CommentForests.\n",
    "    \"\"\"\n",
    "    # Dict of dicts with format {comment_id : comment_info_dict}\n",
    "    comment_data: Dict[str, Dict[str, Any]] = {}\n",
    "    # Update comments dict with info dict \n",
    "    for comment in fetch_comments(submission, limit=limit):\n",
    "        record = {\n",
    "            'comment_id':comment.id,\n",
    "            'body':comment.body,\n",
    "            'score':comment.score,\n",
    "            'timestamp':comment.created_utc,\n",
    "            'subreddit':comment.subreddit_name_prefixed,\n",
    "            'parent_submission_id':submission.id\n",
    "        }\n",
    "        # Stream\n",
    "        yield \"comment\", record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_submissions_and_comments(subreddit_name:str, query:str, limit:int=50, **search_kwargs):\n",
    "    \"\"\"\n",
    "    Fetches submissions, parses each submission into a dictionary record, and calls the stream_comments\n",
    "    function on each submission. Submission data and comment data are streamed for efficient memory \n",
    "    footprint when handling larger datasets. \n",
    "    \"\"\"\n",
    "    sub = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Fetch submissions, and for every submission, fetch the comments\n",
    "    for submission in fetch_submissions(**search_kwargs, subreddit=sub, query=query, limit=limit):\n",
    "        # Update submissions dict with info dict from submission\n",
    "        record = {\n",
    "            'submission_id':submission.id,\n",
    "            'title':submission.title,\n",
    "            'selftext':submission.selftext,\n",
    "            'score':submission.score,\n",
    "            'upvote_ratio':submission.upvote_ratio,\n",
    "            'timestamp':submission.created_utc,\n",
    "            'subreddit':submission.subreddit_name_prefixed,\n",
    "            'num_comments':submission.num_comments\n",
    "            }\n",
    "        # Stream comment data from current submission (\"submission\", Dict[str, Any])\n",
    "        yield from stream_comments(submission)\n",
    "        # Stream submission data\n",
    "        yield \"submission\", record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9cf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_aggregate_results(subreddits:List[str], \n",
    "                             queries:List[str],\n",
    "                             max_requests:int=100, \n",
    "                             min_requests:int=50,\n",
    "                             period:float=60.0,\n",
    "                             jitter:List[float] = [1.0,10.0],\n",
    "                             **search_kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for streaming functions. Takes a list of subreddits and queries, then calls the \n",
    "    stream_search_results  function for each combination of subreddit and query. Jitter is implemented \n",
    "    to introduce randomness in number of API requests with a short backoff in each iteration to ensure\n",
    "    adherence to Reddit API rate limits.\n",
    "    \"\"\"\n",
    "    assert isinstance(subreddits, list), \"Argument 'subreddits' expects a list of subreddit names.\"\n",
    "    assert isinstance(queries, list), \"Argument 'queries' expects a list of search queries names.\"\n",
    "    \n",
    "    # API request counter for triggering execution cooldown\n",
    "    Trace = namedtuple('Trace', ['timestamp','total_requests'])\n",
    "    trace_requests = []\n",
    "    total_requests = 0\n",
    "    \n",
    "    # Parse submission and comment data with jittered API calls\n",
    "    for subreddit, query in product(subreddits, queries):\n",
    "        # Random number of requests per iteration to reduce predictability\n",
    "        submission_limit = int(random.uniform(min_requests, max_requests))\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = dt.datetime.now()\n",
    "        \n",
    "        # Stream data \n",
    "        yield from stream_submissions_and_comments(**search_kwargs, subreddit_name=subreddit, query=query, limit=submission_limit)\n",
    "        \n",
    "        # Record end time and compare against window (start + 1 minute)\n",
    "        end_time = dt.datetime.now()\n",
    "        \n",
    "        # Trigger delay to ensure average requests are under 100 per minute\n",
    "        window = start_time + dt.timedelta(seconds=period)\n",
    "        delay = abs(window - end_time).seconds + random.uniform(*jitter)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Update total requests and request history\n",
    "        total_requests += submission_limit\n",
    "        trace_requests.append(Trace(dt.datetime.now(), total_requests))\n",
    "    \n",
    "    print(f'Finished writing data to disk.\\nTotal requests made: {sum(trace_requests)};\\nTrace: {\"\\n\".join(trace_requests)}.')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22800dc1",
   "metadata": {},
   "source": [
    "### Text File Parser for Subreddit and Search Queries\n",
    "\n",
    "`parse_txt_file`: Parses text files containing data separated by newlines. Returns a list. Used for containerizing search_queries and subreddit strings into separate text files that can be easily mutated without modifying source code.\n",
    "<blockquote>\n",
    "\n",
    "__Input:__\n",
    "- String for the path of text file, with each item separated by a newline\n",
    "\n",
    "__Output:__\n",
    "- List (e.g. search queries, subreddit names)\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_file(file_path:str):\n",
    "    \"\"\"\n",
    "    Utility function for parsing a multi-line text file where each item is separated\n",
    "    by a newline.\n",
    "    Input: String for file path\n",
    "    Output: List\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Ignore comments and empty lines\n",
    "        results = [line.rstrip(\"\\n\") for line in f if not (line.startswith('#') or line.startswith(\"\\n\"))]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1817cc5",
   "metadata": {},
   "source": [
    "### Fetching and parsing search results from Reddit used car communities\n",
    "\n",
    "To scrape the relevant text data from Reddit, I created a small list of queries covering diverse yet relevant topics to buying affordable used vehicles. The queries involved location-specific, model-specific, and thematic keywords to ensure that the search covers as much ground as possible. Chosen subreddits have > 1e5 subscribers to ensure that search queries will yield a significant amount of results per API request.\n",
    "\n",
    "With a 10x10 query and subreddit array, I expect at least an initial 100 requests for the subreddit search yielding 100x100 submissions at most.\n",
    "\n",
    "Fetching the comments involves significantly more requests as each submission requires 1 request to yield the CommentForest. Fetching the comments will require at least 10,000 requests.\n",
    "\n",
    "__Expected Minimum API Requests__\n",
    "|Search Requests|Comment Fetch Requests|Total Requests|\n",
    "|:----------|:----------|:----------|\n",
    "|100      |10,000  |10,100|\n",
    "\n",
    "As such, a single batch job covering all query-subreddit combinations will yield at least 10,100 API requests in a single go, which wildly exceeds the Reddit API fair use policy (i.e. Cap requests to 100/min averaged over 10-minute sliding window). To address this issue, batched processing will be implemented to ensure average requests is under safe rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b06d7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse text files containing search queries and subreddit names\n",
    "search_queries = parse_txt_file(\"../src/search_queries.txt\")\n",
    "subreddits = parse_txt_file(\"../src/subreddits.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "193078da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Search Pairs-------\n",
      "CarsAustralia - affordable reliable used cars under 15k Australia\n",
      "CarsAustralia - affordable reliable used cars under 10k USA\n",
      "UsedCars - affordable reliable used cars under 15k Australia\n",
      "UsedCars - affordable reliable used cars under 10k USA\n"
     ]
    }
   ],
   "source": [
    "search_queries = search_queries[:2]\n",
    "subreddits = subreddits[:2]\n",
    "print('-------Search Pairs-------')\n",
    "for (subreddit, query) in product(subreddits, search_queries):\n",
    "    print(subreddit,\"-\",query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "dcb454e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooldown triggered: sleeping for 109.40741414375071s to avoid rate limit.\n",
      "Cooldown triggered: sleeping for 71.57586797057071s to avoid rate limit.\n",
      "Cooldown triggered: sleeping for 65.00197525949584s to avoid rate limit.\n",
      "CPU times: user 2.8 s, sys: 314 ms, total: 3.11 s\n",
      "Wall time: 7min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fetch search results and parse to dict of dicts\n",
    "submission_data, comment_data = aggregate_search_results(subreddits=subreddits, queries=search_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528009c",
   "metadata": {},
   "source": [
    "## Storing the scraped data\n",
    "\n",
    "### Formatting to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "cf8279db",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame.from_dict(submission_data, orient='index')\n",
    "comment_df = pd.DataFrame.from_dict(comment_data, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f22123",
   "metadata": {},
   "source": [
    "### Exporting DataFrame to a Parquet file for efficient storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5c641d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_parquet(os.path.join(\"..\",\"data\",\"submission_data.parquet\"), \n",
    "                         engine='pyarrow',\n",
    "                         compression='gzip')\n",
    "\n",
    "comment_df.to_parquet(os.path.join(\"..\",\"data\",\"comment_data.parquet\"),\n",
    "                      engine='pyarrow',\n",
    "                      compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dba01",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42fa6a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b416961",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limiter = RateLimiter(max_calls=3, period=2, jitter=[1,2])\n",
    "\n",
    "def bar():\n",
    "    yield from foo()\n",
    "\n",
    "def foo():\n",
    "    for j in [i for i in range(15)]:\n",
    "        rate_limiter.wait_for_slot(n_request=1)\n",
    "        yield j\n",
    "        \n",
    "for k in bar():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ff7ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
