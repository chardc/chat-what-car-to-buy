{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d1e5ad",
   "metadata": {},
   "source": [
    "# Data Pipeline Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a7928",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044418d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas praw prawcore python-dotenv pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a9c3309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw, prawcore, praw.models\n",
    "import time, os, sys, functools, random, json\n",
    "import datetime as dt\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional, List, Dict, Any, Union, Tuple, NamedTuple\n",
    "from itertools import product\n",
    "from collections import deque, namedtuple\n",
    "from collections.abc import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bb44f",
   "metadata": {},
   "source": [
    "## Setting up access to Reddit API\n",
    "Access keys to Reddit API are stored in a .env file under the config directory of this repository. A template for the .env file is provided in the config directory.\n",
    "\n",
    "The config.py script assigns the environment variables to the `PRAW_ID`, `PRAW_SECRET`, `PRAW_USER_AGENT`, `PRAW_USERNAME`, and `PRAW_PASSWORD` global variables respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c3ec55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file for access keys\n",
    "module_path = '../src'\n",
    "load_dotenv(os.path.join(module_path,'config','.env'))\n",
    "\n",
    "# Import config.py to access environment variables\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from usedcaranalytics.config.api import PRAW_ID, PRAW_SECRET, PRAW_USER_AGENT, PRAW_USERNAME, PRAW_PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "de3a53ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded API keys and login credentials.\n"
     ]
    }
   ],
   "source": [
    "if all((PRAW_ID, PRAW_SECRET, PRAW_USER_AGENT, PRAW_USERNAME, PRAW_PASSWORD)):\n",
    "    print('Successfully loaded API keys and login credentials.')\n",
    "else:\n",
    "    raise Exception('Reddit API keys and login credentials unsuccessfully loaded. Retry the script.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "474a926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRAW \n",
    "REDDIT = praw.Reddit(\n",
    "    client_id=PRAW_ID,\n",
    "    client_secret=PRAW_SECRET,\n",
    "    username=PRAW_USERNAME,\n",
    "    password=PRAW_PASSWORD,\n",
    "    user_agent=PRAW_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01d930",
   "metadata": {},
   "source": [
    "## Extracting text data\n",
    "\n",
    "This section deals with the process of extracting and storing text data and metadata from Reddit posts and comments. My objective is to present my thought process and design principles in implementing the data pipeline for this project.\n",
    "\n",
    "__Data Pipeline Overview:__\n",
    "1. Establish access to Reddit API\n",
    "2. Crawl predefined subreddits by searching submissions using predefined queries\n",
    "3. Extract textual data and metadata from relevant posts and child comments\n",
    "4. Preprocess data (Optional)\n",
    "5. Store extracted data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ca8f5",
   "metadata": {},
   "source": [
    "### The Challenge of Reddit API Rate Limiting\n",
    "\n",
    "__Building the search query and subreddit pairs__\n",
    "\n",
    "To scrape the relevant text data from Reddit, I created a small list of queries covering diverse yet relevant topics to buying affordable used vehicles. The queries involved location-specific, model-specific, and thematic keywords to ensure that the search covers as much ground as possible. Chosen subreddits have > 1e5 subscribers to ensure that search queries will yield a significant amount of results per API request. These queries and subreddits can be accessed in the paths: `../src/search_queries.txt` and `../src/subreddits.txt`, respectively. \n",
    "\n",
    "__Searching relevant posts per Subreddit__\n",
    "\n",
    "The objective is to search and scrape for posts (and child comments) within the specified subreddits using the search queries provided. However, with a 10x10 query and subreddit array, I expect at least an initial 100 requests for the subreddit search yielding 100x100 submissions at most. Fetching the comments involves significantly more requests as each submission requires 1 request to yield the CommentForest. Fetching the comments will require at least 10,000 requests.\n",
    "\n",
    "__Expected Minimum API Requests__\n",
    "|Search Requests|Comment Fetch Requests|Total Requests|\n",
    "|:----------|:----------|:----------|\n",
    "|100      |10,000  |10,100|\n",
    "\n",
    "From the table above, a single batch job covering all query-subreddit combinations will yield at least 10,100 API requests in a single go, which wildly exceeds the Reddit API fair use policy (i.e. Cap requests to 100/min averaged over 10-minute sliding window). \n",
    "\n",
    "__Implementing a sliding window request counter and backoff algorithms__\n",
    "\n",
    "To ensure the script adheres to fair use policies, I implemented two-pronged fail-safe logic:\n",
    "1. Handle transient failures for each API request by implementing a backoff algorithm\n",
    "2. Mitigate the risk of #1 happening by implementing a program-level API request counter that tracks current and expected calls within a specified sliding window. This rate limiter will throttle requests until there's an available slot.\n",
    "\n",
    "__Read more:__\n",
    "1. [API Rate Limits Explained: Best Practices for 2025](https://orq.ai/blog/api-rate-limit)\n",
    "2. [Exponential Backoff And Jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)\n",
    "3. [Yield Statements vs. Returning Lists in Python](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://community.aws/content/2h01Byx1ytU8357tp2bvcUuJ2j0/yield-statements-vs-returning-lists-in-python%23:~:text%3DYield%253A%2520Ideal%2520for%2520large%2520data,potentially%2520leading%2520to%2520memory%2520errors.&ved=2ahUKEwjzvJvd74uOAxVkQ6QEHVAVMHcQFnoECBIQAw&usg=AOvVaw3hMoJHnPwBIQOdBmB_NiBD)\n",
    "4. [Rate Limiter - Sliding Window Counter](https://medium.com/@avocadi/rate-limiter-sliding-window-counter-7ec08dbe21d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389816fa",
   "metadata": {},
   "source": [
    "##### Rate Limiter Class\n",
    "\n",
    "The RateLimiter class is initialized at the beginning of the script and is used to track API requests made within a specific sliding window. Requests are throttled when total expected requests go beyond rate limits (Reddit = 100/min) for the current window. Jitter is injected to the wait time and a random buffer for requests is left to avoid coasting at rate limits (abuse avoidance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0ed54127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRAW auth limits checker implementation\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, reddit:praw.Reddit, buffer_range:Tuple[int]=(50,100)):\n",
    "        self.REDDIT = reddit\n",
    "        self.BUFFER_RANGE = buffer_range\n",
    "        self.total_requests = 0\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"Checks if current request can be accommodated based on current limits.\"\"\"\n",
    "        # Check remaining requests and randomize buffer to ensure requests dont coast at rate limit\n",
    "        remaining_requests = self.REDDIT.auth.limits['remaining']\n",
    "        buffer = random.randint(*self.BUFFER_RANGE)\n",
    "        \n",
    "        # If we dip into the buffer, sleep until limits reset\n",
    "        if remaining_requests - 1 < buffer:\n",
    "            # Calculate time left until limits refresh and add jitter\n",
    "            reset_ts = self.REDDIT.auth.limits['reset_timestamp']\n",
    "            delay = max(reset_ts - time.time(), 0) + random.randrange(0.01, 5.0)\n",
    "            time.sleep(delay)\n",
    "            # Re-evaluate if API call can proceed\n",
    "            return self.evaluate()\n",
    "        \n",
    "        # Tally API call\n",
    "        self.total_requests += 1\n",
    "        \n",
    "    def print_total_requests(self):\n",
    "        return f'{self.total_requests} total requests as of {dt.datetime.now():%Y-%m-%d %H:%M:%S}.'\n",
    "    \n",
    "    def print_remaining_requests(self):\n",
    "        limits = self.REDDIT.auth.limits\n",
    "        # If limits hasn't refreshed yet, return remaining requests. Otherwise, return total limit\n",
    "        if time.time() < limits['reset_timestamp']:\n",
    "            return limits['remaining_requests']\n",
    "        else:\n",
    "            return limits['remaining_requests'] + limits['used']\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'RateLimiter object: {self.print_remaining_requests()} available requests in current window.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beab4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sliding window request counter implementation\n",
    "\n",
    "# class RateLimiter:\n",
    "#     \"\"\"Rate Limiter with sliding window implementation.\"\"\"\n",
    "#     def __init__(self, max_requests:int=100, period:float=60.0, jitter_seconds:Union[List[float],Tuple[float]]=(0.1,5.0)):\n",
    "#         self.MAX_REQUESTS = max_requests\n",
    "#         self.PERIOD = period\n",
    "#         self.JITTER_SECONDS = jitter_seconds\n",
    "#         self._requests_in_window = deque()\n",
    "#         self.total_requests = 0\n",
    "        \n",
    "#     def wait_for_slot(self, n_request:int=1) -> None:\n",
    "#         \"\"\"\n",
    "#         Delays execution of subsequent API request or code chunk to ensure maximum\n",
    "#         function calls or request adheres to rate limits within a specified window.\n",
    "#         \"\"\"\n",
    "#         window_end = time.time()\n",
    "#         window_start = window_end - self.PERIOD\n",
    "        \n",
    "#         # Remove older batches when timestamp is out of current window\n",
    "#         while self._requests_in_window and self._requests_in_window[0] < window_start:\n",
    "#             self._requests_in_window.popleft()\n",
    "        \n",
    "#         # Check if additional request can be accommodated given requests made in current window\n",
    "#         if len(self._requests_in_window) + n_request > self.MAX_REQUESTS:\n",
    "#             # Wait time is adjusted by jitter\n",
    "#             wait_time = (self._requests_in_window[0] + self.PERIOD) - window_end\n",
    "#             time.sleep(max(wait_time + random.uniform(*self.JITTER_SECONDS), 0))\n",
    "#             # Re-run the function and determine if request can be accommodated\n",
    "#             return self.wait_for_slot(n_request)\n",
    "        \n",
    "#         # Enqueue current request to trace requests\n",
    "#         for _ in range(n_request):\n",
    "#             self._requests_in_window.append(time.time())\n",
    "#             self.tally_request()\n",
    "    \n",
    "#     def tally_request(self):\n",
    "#         \"\"\"Tallies current request and stores it in instance memory.\"\"\"\n",
    "#         self.total_requests += 1\n",
    "#         return self\n",
    "    \n",
    "#     def print_total_requests(self):\n",
    "#         return f'{self.total_requests} total requests as of {dt.datetime.now():%Y-%m-%d %H:%M:%S}.'\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         param_dict = {\n",
    "#             'max_requests' : self.MAX_REQUESTS,\n",
    "#             'period' : self.PERIOD,\n",
    "#             'jitter_seconds' : self.JITTER_SECONDS\n",
    "#             }\n",
    "#         return f'RateLimiter({\", \".join([f'{k}={v}' for k, v in param_dict.items()])})'\n",
    "    \n",
    "#     def __get__(self):\n",
    "#         return {\n",
    "#             'MAX_REQUESTS':self.MAX_REQUESTS, \n",
    "#             'PERIOD':self.PERIOD, \n",
    "#             'JITTER_SECONDS':self.JITTER_SECONDS, \n",
    "#             '_requests_in_window': self._requests_in_window, \n",
    "#             'total_requests': self.total_requests\n",
    "#             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bda2edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize rate limiter\n",
    "rate_limiter = RateLimiter(REDDIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3feb418",
   "metadata": {},
   "source": [
    "##### Decorator Factory for implementing Exponential Backoff and Full Jitter\n",
    "The function below creates a flexible decorator that can be adjusted based on the intended maximum retries, exponential backoff caps, and inclusion of jitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8a6d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_on_rate_limit(max_retries:int=5, \n",
    "                        base_delay:float=1.0, \n",
    "                        cap_delay:float=60.0, \n",
    "                        jitter:bool=True):\n",
    "    \"\"\"\n",
    "    Decorator factory that applies exponential backoff (with optional jitter) when Reddit API\n",
    "    rate limits (HTTP 429) or server errors occur. Stops after max_retries and re-raises the exception.\n",
    "    \n",
    "    Input:\n",
    "        - Integer value for max retries. When attempts exceed this number, an Exception is raised\n",
    "        - Float for base delay in seconds (i.e. Delay at first failed attempt)\n",
    "        - Float for maximum delay in seconds\n",
    "        - Bool on whether to implement full jitter or not\n",
    "    Output:\n",
    "        - Decorator to be applied to an PRAW API request wrapper\n",
    "    \"\"\"\n",
    "    def decorator(func):# -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]:\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Start with base delay, then exponentially scale by attempt\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except prawcore.exceptions.ResponseException as e:\n",
    "                    if attempt > max_retries:\n",
    "                        raise Exception(\"Max retries exceeded with Reddit API.\")\n",
    "                    delay = min(cap_delay, base_delay * 2 ** attempt)\n",
    "                    if jitter:\n",
    "                        delay = random.uniform(0, delay)\n",
    "                    print(f\"[WARNING] {e.__class__.__name__} on attempt {attempt+1}, retrying after {delay:.2f}s.\")\n",
    "                    time.sleep(delay)\n",
    "                    attempt += 1\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d4d07",
   "metadata": {},
   "source": [
    "#### API Call Wrappers with Backoff Algorithms\n",
    "\n",
    "The helper functions were designed to extract relevant data and metadata from Reddit submissions and comments, and package the data into a dict of dicts that can be easily parsed into a Pandas DataFrame object for further analysis. The backoff decorator is applied to each API call wrapper to handle transient errors raised by HTTP 429 response (Too Many Requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "05846dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff_on_rate_limit()\n",
    "def fetch_submissions(subreddit:object, query:str, limit:int=100, **kwargs):\n",
    "    \"\"\"Modify the subreddit search from PRAw to ensure adherence to safe request limits.\"\"\"\n",
    "    # Record API request and throttle if needed\n",
    "    rate_limiter.evaluate()\n",
    "    return subreddit.search(**kwargs, query=query, limit=limit)\n",
    "\n",
    "@backoff_on_rate_limit()\n",
    "def fetch_comments(submission:object, limit:int=0):\n",
    "    \"\"\"Modify the comment fetch from PRAW to ensure adherence to safe request limits.\"\"\"\n",
    "    # Record API request and throttle if needed\n",
    "    rate_limiter.evaluate()\n",
    "    # Replace 'more' with specified limit (default = 0 or retain top-level comments only)\n",
    "    submission.comments.replace_more(limit=limit)\n",
    "    for comment in submission.comments:\n",
    "        yield comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d0775",
   "metadata": {},
   "source": [
    "#### Text File Parser for Subreddit and Search Queries\n",
    "\n",
    "To limit hardcoding and allow for flexible scraping, I opted to store the list of subreddit names and search queries in their individual text files so that I can simply update that file and re-run the script whenever I need to scrape something rather than digging through the source code whenever I need to modify the search pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e0b7a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_file(file_path:str):\n",
    "    \"\"\"\n",
    "    Utility function for parsing a multi-line text file where each item is separated\n",
    "    by a newline.\n",
    "    \n",
    "    Input:\n",
    "        - String for the path of text file, with each item separated by a newline\n",
    "    Output:\n",
    "        - List (e.g. search queries, subreddit names)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Ignore comments and empty lines\n",
    "        results = [line.rstrip(\"\\n\") for line in f if not (line.startswith('#') or line.startswith(\"\\n\"))]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "98f7a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_pairs(\n",
    "    project_root:str, \n",
    "    subdir:str='data/raw', \n",
    "    file_names:Union[Tuple[str],List[str]]=('search_queries.txt','subreddits.txt')\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Utility function that returns tuple of lists of subreddit names and search queries.\n",
    "    Input:\n",
    "        - project root path\n",
    "        - subdirectory path that contains the files\n",
    "        - file names in strict order: 1) search queries, 2) subreddits\n",
    "    Output:\n",
    "        - Tuple(list of queries, list of subreddits)\n",
    "    \"\"\"\n",
    "    \n",
    "    query_fp, subreddit_fp = tuple(\n",
    "        os.path.join(project_root, subdir, fname) for fname in file_names\n",
    "        )\n",
    "    \n",
    "    return tuple(parse_txt_file(fp) for fp in (query_fp, subreddit_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2dc7a",
   "metadata": {},
   "source": [
    "### Data Streaming\n",
    "\n",
    "__Rationale: Scalability of Scraping Logic__\n",
    "\n",
    "Previously, I explored building dictionaries within each scraping function and returning that dictionary to the data storage logic. However, this approach doesn't scale well since device memory may become a bottleneck with larger volumes of API calls. From my research, it's recommended to use generators to stream data from APIs as memory overhead is limited to the data extracted from the most recent call.\n",
    "\n",
    "__Data Extraction Overview__:\n",
    "\n",
    "1. Initialize the rate limiter class to keep track of requests within a 60-second sliding window.\n",
    "2. Parse the text files containing subreddit names and search queries, then get the combination of search pairs.\n",
    "3. Given a search pair, initialize a Subreddit class and search relevant submissions within that subreddit.\n",
    "4. For every relevant submission, extract relevant data from Submission class attributes and stream a tuple of record type and submission data dictionary\n",
    "5. Subsequently, for every submission, request the top-level comments, and for each top-level comment, stream a tuple of record type and comment data dictionary.\n",
    "\n",
    "#### Data Streaming Functions\n",
    "1. Streamer for comments given a praw.models.Submission object.\n",
    "2. Streamer for submission data and child comments. This is a wrapper for the comment streamer that takes a single search pair as input, streams all relevant submissions and child comments.\n",
    "3. Wrapper for #2 and takes a list of subreddits and search queries and inputs the search pair combinations into the submission and comment streaming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "acf4f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_comments(submission:object, limit:int=0):\n",
    "    \"\"\"\n",
    "    Fetches comments from a Submission objects then parses each comment into a dictionary record.\n",
    "    Each entry is streamed for efficient memory footprint when handling larger CommentForests.\n",
    "    \n",
    "    Input:\n",
    "        - Submission object from PRAW (i.e. Reddit posts)\n",
    "        - Integer for .replace_more limit parameter, default=0 (i.e. top/parent comments only)\n",
    "    Output:\n",
    "        - Dict of comments in the format {comment_id : {data_header: data_value}}  \n",
    "    \"\"\"\n",
    "    assert (isinstance(limit, int) and limit >= 0) or limit is None, 'Limit must be an integer >= 0 or None.'\n",
    "    \n",
    "    # Update comments dict with info dict \n",
    "    for comment in fetch_comments(submission, limit=limit):\n",
    "        \n",
    "        # Stream comment data when slot available in current window\n",
    "        yield \"comment\", {\n",
    "            'comment_id':comment.id,\n",
    "            'body':comment.body,\n",
    "            'score':comment.score,\n",
    "            'timestamp':int(comment.created_utc),\n",
    "            'subreddit':comment.subreddit_name_prefixed,\n",
    "            'parent_submission_id':submission.id\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ff8547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_submissions_and_comments(subreddit_name:str, query:str, limit:int=50, **search_kwargs):\n",
    "    \"\"\"\n",
    "    Fetches submissions, parses each submission into a dictionary record, and calls the stream_comments\n",
    "    function on each submission. Submission data and comment data are streamed for efficient memory \n",
    "    footprint when handling larger datasets. \n",
    "    \n",
    "    Input:\n",
    "        - String of Subreddit name\n",
    "        - String of search query\n",
    "        - Integer for limit of submissions yielded by PRAW subreddit search\n",
    "    Output:\n",
    "        - Tuple of submission data dict and comment data dict\n",
    "    \"\"\"\n",
    "    assert isinstance(subreddit_name, str), 'Subreddit name must be a string.'\n",
    "    assert isinstance(query, str), 'Search query must be a string.'\n",
    "    assert isinstance(limit, int) and limit > 0, 'Limit must be a positive non-zero integer.'\n",
    "    \n",
    "    SUB = REDDIT.subreddit(subreddit_name)\n",
    "    \n",
    "    # Fetch submissions, and for every submission, fetch the comments\n",
    "    for submission in fetch_submissions(**search_kwargs, subreddit=SUB, query=query, limit=limit):\n",
    "        # Stream comment data from current submission (\"submission\", Dict[str, Any])\n",
    "        yield from stream_comments(submission)\n",
    "        \n",
    "        # Stream submission data when slot available in current window\n",
    "        yield \"submission\", {\n",
    "            'submission_id':submission.id,\n",
    "            'title':submission.title,\n",
    "            'selftext':submission.selftext,\n",
    "            'score':submission.score,\n",
    "            'upvote_ratio':submission.upvote_ratio,\n",
    "            'timestamp':int(submission.created_utc),\n",
    "            'subreddit':submission.subreddit_name_prefixed,\n",
    "            'num_comments':submission.num_comments\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "de9cf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_aggregate_results(subreddits:List[str], queries:List[str],**search_kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for streaming functions. Takes a list of subreddits and queries, then calls the \n",
    "    stream_search_results  function for each combination of subreddit and query. \n",
    "    \n",
    "    Input:\n",
    "        - List of subreddit name strings\n",
    "        - List of search query strings\n",
    "        - Int of maximum requests per minute, also determines upper bound of search result limit\n",
    "        - Int of minimum requests, which is the floor of search result limit\n",
    "        - Float of seconds denoting the time period for counting the API call limits\n",
    "        - List of float values of seconds to randomly add to interval delay \n",
    "    Output:\n",
    "        - Tuple of aggregated submissions dict and comments dict\n",
    "    \"\"\"\n",
    "    assert isinstance(subreddits, list), \"Argument 'subreddits' expects a list of subreddit names.\"\n",
    "    assert isinstance(queries, list), \"Argument 'queries' expects a list of search queries names.\"\n",
    "    \n",
    "    # Parse submission and comment data with jittered API calls\n",
    "    for subreddit, query in product(subreddits, queries):\n",
    "        # Stream submission and comment records (str(record_type), Dict[str(col_name), Any])\n",
    "        yield from stream_submissions_and_comments(**search_kwargs, subreddit_name=subreddit, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d7dea",
   "metadata": {},
   "source": [
    "#### Testing the streaming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8e580bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 769 ms, sys: 84.4 ms, total: 853 ms\n",
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test the streamer\n",
    "sample_stream = tuple(\n",
    "    stream_aggregate_results(\n",
    "        ['cars','CarsAustralia'],\n",
    "        ['toyota corolla first car','mazda 3 issues'],\n",
    "        limit=10\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "47ca44b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'44 total requests as of 2025-06-27 20:33:08.'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print total requests made so far\n",
    "rate_limiter.print_total_requests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "daed3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregate data then build dataframes\n",
    "comment_data = tuple(data for record, data in sample_stream if record == 'comment')\n",
    "submission_data = tuple(data for record, data in sample_stream if record == 'submission')\n",
    "comment_df = pd.DataFrame(comment_data)\n",
    "submission_df = pd.DataFrame(submission_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7f67c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1688, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>parent_submission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ioe046j</td>\n",
       "      <td>The GR is going to help Toyota sell the regula...</td>\n",
       "      <td>167</td>\n",
       "      <td>1663158970</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iodt8kt</td>\n",
       "      <td>Lots of talk about it being 'safe' and 'stable...</td>\n",
       "      <td>93</td>\n",
       "      <td>1663155458</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iodw9bg</td>\n",
       "      <td>Will be interesting to see if this car or any ...</td>\n",
       "      <td>65</td>\n",
       "      <td>1663157124</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iodun0d</td>\n",
       "      <td>Now I’m ready for the Civic Type R, WRX, Elant...</td>\n",
       "      <td>99</td>\n",
       "      <td>1663156249</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ioenkne</td>\n",
       "      <td>A $50k crazy Corolla is cool and all, but can'...</td>\n",
       "      <td>47</td>\n",
       "      <td>1663168836</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>xdyy00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id                                               body  score  \\\n",
       "0    ioe046j  The GR is going to help Toyota sell the regula...    167   \n",
       "1    iodt8kt  Lots of talk about it being 'safe' and 'stable...     93   \n",
       "2    iodw9bg  Will be interesting to see if this car or any ...     65   \n",
       "3    iodun0d  Now I’m ready for the Civic Type R, WRX, Elant...     99   \n",
       "4    ioenkne  A $50k crazy Corolla is cool and all, but can'...     47   \n",
       "\n",
       "    timestamp subreddit parent_submission_id  \n",
       "0  1663158970    r/cars               xdyy00  \n",
       "1  1663155458    r/cars               xdyy00  \n",
       "2  1663157124    r/cars               xdyy00  \n",
       "3  1663156249    r/cars               xdyy00  \n",
       "4  1663168836    r/cars               xdyy00  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(comment_df.shape)\n",
    "comment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e087ebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xdyy00</td>\n",
       "      <td>Toyota GR Corolla | First Drive, Leveling Expe...</td>\n",
       "      <td></td>\n",
       "      <td>436</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1663153268</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51ej3q</td>\n",
       "      <td>You are sent back in time to when Henry Ford i...</td>\n",
       "      <td>Id send him a 1999 Toyota Levin with 400,000km...</td>\n",
       "      <td>539</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1473157092</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qpinpq</td>\n",
       "      <td>Toyota has some huge potential with the GR Cor...</td>\n",
       "      <td>For the first time in my life, I think I will ...</td>\n",
       "      <td>385</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1636393029</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15zrb24</td>\n",
       "      <td>Is it just me or is the Toyota corolla the mos...</td>\n",
       "      <td>So I today I had the (dis)pleasure of renting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1692851066</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x64gh1</td>\n",
       "      <td>2023 Toyota GR Corolla - Circuit Edition vs. M...</td>\n",
       "      <td></td>\n",
       "      <td>250</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1662344567</td>\n",
       "      <td>r/cars</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id                                              title  \\\n",
       "0        xdyy00  Toyota GR Corolla | First Drive, Leveling Expe...   \n",
       "1        51ej3q  You are sent back in time to when Henry Ford i...   \n",
       "2        qpinpq  Toyota has some huge potential with the GR Cor...   \n",
       "3       15zrb24  Is it just me or is the Toyota corolla the mos...   \n",
       "4        x64gh1  2023 Toyota GR Corolla - Circuit Edition vs. M...   \n",
       "\n",
       "                                            selftext  score  upvote_ratio  \\\n",
       "0                                                       436          0.93   \n",
       "1  Id send him a 1999 Toyota Levin with 400,000km...    539          0.87   \n",
       "2  For the first time in my life, I think I will ...    385          0.89   \n",
       "3  So I today I had the (dis)pleasure of renting ...      0          0.31   \n",
       "4                                                       250          0.92   \n",
       "\n",
       "    timestamp subreddit  num_comments  \n",
       "0  1663153268    r/cars           251  \n",
       "1  1473157092    r/cars           545  \n",
       "2  1636393029    r/cars           244  \n",
       "3  1692851066    r/cars           183  \n",
       "4  1662344567    r/cars           112  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(submission_df.shape)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74c3e8",
   "metadata": {},
   "source": [
    "#### Consolidating the streaming functions into a DataStreamer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "07579db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStreamer:\n",
    "    def __init__(self, reddit:praw.Reddit):\n",
    "        self.REDDIT = reddit\n",
    "        self.rate_limiter = RateLimiter(REDDIT)\n",
    "    \n",
    "    @backoff_on_rate_limit()\n",
    "    def _fetch_submissions(self, subreddit:praw.models.Subreddit, query:str, **kwargs):\n",
    "        \"\"\"Modify the subreddit search from PRAw to ensure adherence to safe request limits.\"\"\"\n",
    "        # Evaluate if current request can be accommodated with remaining limits\n",
    "        self.rate_limiter.evaluate()\n",
    "        return subreddit.search(**kwargs, query=query)\n",
    "\n",
    "    @backoff_on_rate_limit()\n",
    "    def _fetch_comments(self, submission:praw.models.Submission, limit:int=0):\n",
    "        \"\"\"Modify the comment fetch from PRAW to ensure adherence to safe request limits.\"\"\"\n",
    "        # Evaluate if current request can be accommodated with remaining limits\n",
    "        self.rate_limiter.evaluate()\n",
    "        # Replace 'more comments' with specified limit (default = 0 or retain top-level comments only)\n",
    "        submission.comments.replace_more(limit=limit)\n",
    "        for comment in submission.comments:\n",
    "            yield comment\n",
    "    \n",
    "    def _stream_comments(self, submission:praw.models.Submission, limit:int=0):\n",
    "        \"\"\"\n",
    "        Fetches comments from a Submission objects then parses each comment into a dictionary record.\n",
    "        Each entry is streamed for efficient memory footprint when handling larger CommentForests.\n",
    "        \"\"\"\n",
    "        # Update comments dict with info dict \n",
    "        for comment in self._fetch_comments(submission, limit=limit):            \n",
    "            # Stream comment data when slot available in current window\n",
    "            yield \"comment\", {\n",
    "                'comment_id':comment.id,\n",
    "                'body':comment.body,\n",
    "                'score':comment.score,\n",
    "                'timestamp':int(comment.created_utc),\n",
    "                'subreddit':comment.subreddit_name_prefixed,\n",
    "                'parent_submission_id':submission.id\n",
    "                }\n",
    "    \n",
    "    def stream_search_results(self, subreddit_name:str, query:str, limit:int=50, **search_kwargs):\n",
    "        \"\"\"\n",
    "        Fetches submissions then fetches the comments for each submission. Data is then repackaged \n",
    "        into a dictionary and streamed as (str(record type), Dict[str(column name), Any(data)]).\n",
    "        \"\"\"\n",
    "        subreddit = self.REDDIT.subreddit(subreddit_name)\n",
    "        # Fetch submissions, and for every submission, fetch the comments\n",
    "        for submission in self._fetch_submissions(**search_kwargs, subreddit=subreddit, query=query, limit=limit):\n",
    "            # Stream comment data from current submission (\"submission\", Dict[str, Any])\n",
    "            yield from self._stream_comments(submission)\n",
    "            # Stream submission data when slot available in current window\n",
    "            yield \"submission\", {\n",
    "                'submission_id':submission.id,\n",
    "                'title':submission.title,\n",
    "                'selftext':submission.selftext,\n",
    "                'score':submission.score,\n",
    "                'upvote_ratio':submission.upvote_ratio,\n",
    "                'timestamp':int(submission.created_utc),\n",
    "                'subreddit':submission.subreddit_name_prefixed,\n",
    "                'num_comments':submission.num_comments\n",
    "                }\n",
    "            \n",
    "    def stream(self, subreddits:List[str], queries:List[str],**search_kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper for streaming functions. Takes a list of subreddits and queries, then calls the \n",
    "        stream_search_results method for each combination of subreddit and query. \n",
    "        \"\"\"\n",
    "        # Parse submission and comment data with jittered API calls\n",
    "        for subreddit, query in product(subreddits, queries):\n",
    "            # Stream submission and comment records (str(record_type), Dict[str(col_name), Any])\n",
    "            yield from self.stream_search_results(**search_kwargs, subreddit_name=subreddit, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61351cc3",
   "metadata": {},
   "source": [
    "### Transforming the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class DataTransformer(Metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def __call__(self, table:pa.Table):\n",
    "        \"\"\"Transform current batch table before writing to disk.\"\"\"\n",
    "        # TO BE IMPLEMENTED\n",
    "        out_table = self.do_something(table)\n",
    "        return out_table\n",
    "    \n",
    "    @abstractmethod\n",
    "    @staticmethod\n",
    "    def add_date_column(table):\n",
    "        \"\"\"Convert unix timestamp to YYYY-MM-DD format and insert as column.\"\"\"\n",
    "        out_table = self.do_something(table)\n",
    "        return out_table\n",
    "    \n",
    "    @abstractmethod\n",
    "    @staticmethod\n",
    "    def filter_comments(table):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    @staticmethod\n",
    "    def filter_submissions(table):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbac01",
   "metadata": {},
   "source": [
    "## Storing the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1709b1",
   "metadata": {},
   "source": [
    "#### Data Storage\n",
    "\n",
    "Data will be stored as Pyarrow tables since Parquet files have higher compression rates resulting in smaller memory footprint, which is beneficial for larger datasets.\n",
    "\n",
    "__Data Storage Logic:__\n",
    "1. Store the file paths for submission data and comment data Parquet files\n",
    "2. Define the schema for the parquet files to preserve data type on export, marginally improve write performance, and avoid silent errors\n",
    "3. Initialize the data generator with the list of search pairs\n",
    "4. Stream data from the generator and store each record in a dictionary buffer. When buffer size reaches target byte size, convert to Pyarrow Table and write to dataset directory as Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4941df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetDataLoader:\n",
    "    def __init__(self, config:Tuple[NamedTuple], target_MB:Union[int,float]=8.0, transformer:object=None):\n",
    "        self.CONFIG = config\n",
    "        self._TRANSFORMER = transformer\n",
    "        self.set_target_mb(target_MB)\n",
    "        self._configure_loader()\n",
    "    \n",
    "    def _configure_loader(self):\n",
    "        \"\"\"Abstracts away the configuration of schemas, buffers, root paths, byte counters.\"\"\"\n",
    "        # Unpack schema from ParquetConfig namedtuples\n",
    "        self._SCHEMAS = {ntuple.record_type : ntuple.schema for ntuple in self.CONFIG}\n",
    "        # Store the dataset directory root paths per record type\n",
    "        self._DATASET_PATHS = {ntuple.record_type : ntuple.dataset_path for ntuple in self.CONFIG}\n",
    "        # Set-up buffers per record type\n",
    "        self._buffers = {\n",
    "            record_type : {\n",
    "                col : [] for col in self._SCHEMAS[record_type].names\n",
    "                } \n",
    "            for record_type in self._SCHEMAS\n",
    "            }\n",
    "        # Initialize byte counter per record type\n",
    "        self._buffer_sizes = {record_type : 0 for record_type in self._buffers}\n",
    "        # Batch counter for filename\n",
    "        self._batch_counters = {record_type : 0 for record_type in self._buffers}\n",
    "        return self\n",
    "    \n",
    "    def set_target_mb(self, target_MB:Union[int,float]):\n",
    "        \"\"\"\n",
    "        Setter for target_MB. Updates target_MB and TARGET_BYTES attributes. Allows for updating\n",
    "        buffer size targets post-initialization.\n",
    "        \"\"\"\n",
    "        self.target_MB = target_MB\n",
    "        self._TARGET_BYTES = int(target_MB * 2 ** 20)\n",
    "        return self\n",
    "    \n",
    "    def load(self, data_stream:Generator):\n",
    "        \"\"\"\n",
    "        Streams data (\"record type\", record_dict) from an input generator, stores the data into a \n",
    "        dictionary buffer, and writes to disk when a target byte size or when the function call has \n",
    "        finished.\n",
    "        \"\"\" \n",
    "        # Stream the data, append to buffer, track buffer size, and when target buffer size\n",
    "        # is met, write the record batch to disk and flush the buffer\n",
    "        for record_type, record in data_stream:\n",
    "            buffer = self._buffers[record_type]\n",
    "                \n",
    "            for col in buffer:\n",
    "                buffer[col].append(record.get(col))\n",
    "            \n",
    "            # Update byte count with current record bytes\n",
    "            record_bytes = len(json.dumps(record, separators=(\",\", \":\")).encode(\"utf-8\"))\n",
    "            self._buffer_sizes[record_type] += record_bytes\n",
    "            \n",
    "            # Export parquet files to dataset directory and flush buffers and byte counters            \n",
    "            if self._buffer_sizes[record_type] >= self._TARGET_BYTES:\n",
    "                self._batch_counters[record_type] += 1\n",
    "                self._write(record_type)\n",
    "                buffer, self._buffer_sizes[record_type] = self._flush(buffer)\n",
    "\n",
    "        # Final write for remaining data in both buffers after streaming data\n",
    "        # Only write if there are remaining records to avoid null records in Parquet file\n",
    "        for record_type, buffer in self._buffers.items():\n",
    "            if all(container for container in buffer.values()):\n",
    "                self._write(record_type, buffer)\n",
    "                buffer, self._buffer_sizes[record_type] = self._flush(buffer)\n",
    "                \n",
    "    def _write(self, record_type:str, buffer:Dict[str,List]):\n",
    "        \"\"\"Convert buffer to Pyarrow container, write to Parquet, then flush buffer and byte count.\"\"\"\n",
    "        # File name; Ex. SUBMISSION-0001-20250626-201522\n",
    "        fname = f'{record_type.upper()}-{self._batch_counters:04d}-{dt.datetime.now():%Y%m%d-%H%M%S}.parquet'\n",
    "        # Convert current buffer to Pyarrow Table and write Parquet files to dataset directory\n",
    "        pa_table = pa.Table.from_pydict(buffer)\n",
    "        pq.write_to_dataset(\n",
    "            table=pa_table, \n",
    "            root_path=self._DATASET_PATHS[record_type],\n",
    "            schema=self._SCHEMAS[record_type],\n",
    "            file_name=fname\n",
    "            )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _flush(buffer:Dict[str,List]):\n",
    "        \"\"\"Returns a tuple of empty buffer and byte count, in order, for an input buffer.\"\"\"\n",
    "        # Reconstruct buffer and return with empty values\n",
    "        # Also return 0 for assignment to byte count\n",
    "        return {col : [] for col in buffer}, 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        param_dict = {'config':self.CONFIG, 'target_MB':self.target_MB}\n",
    "        return f'ParquetDataLoader({\", \".join([f'{k}={v}' for k, v in param_dict.items()])})'\n",
    "    \n",
    "    def __get__(self):\n",
    "        return {\n",
    "            'CONFIG' : self.CONFIG,\n",
    "            'target_MB' : self.target_MB,\n",
    "            '_TARGET_BYTES' : self._TARGET_BYTES,\n",
    "            '_SCHEMAS' : self._SCHEMAS,\n",
    "            '_ROOT_PATHS' : self._DATASET_PATHS,\n",
    "            '_buffers' : self._buffers,\n",
    "            '_buffer_sizes' : self._buffer_sizes,\n",
    "            '_batch_counters' : self._batch_counters\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9ab68",
   "metadata": {},
   "source": [
    "#### Abstracting away the DataLoader configuration\n",
    "\n",
    "These utility functions are meant to generate the schemas and dataset directories to generate the ParquetConfig namedtuples used for configuring the ParquetDataLoader. \n",
    "\n",
    "The config variables essentially define the datatypes for submission and comment data when building the PyArrrow tables and exporting to Parquet, and also define the dataset directory where these files will be exported to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b3a4efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize namedtuple for Parquet Config for ease of access and immutability\n",
    "ParquetConfig = namedtuple('ParquetConfig',['record_type','dataset_path','schema'])\n",
    "\n",
    "def load_schema(schema_path:str, record_type:str) -> pa.Schema:\n",
    "    \"\"\"\n",
    "    Returns a pyarrow schema parsed from a JSON file containing the schema.\n",
    "    Input:\n",
    "        - Path to JSON schema. Parent object must be either 'submission' or 'comment'\n",
    "        and child object must be a dictionary containing column name keys and pyarrow datatype values\n",
    "        Ex. 'submission' : {'submission_id' : 'pa.string()'}\n",
    "    Output:\n",
    "        - PyArrow Schema object\n",
    "    \"\"\"\n",
    "    with open(schema_path, 'r') as f:\n",
    "        schemas = json.load(f)\n",
    "        schemas = {key.lower() : value for key, value in schemas.items()}\n",
    "        return pa.schema(\n",
    "            [(col, eval(datatype)) for col, datatype in schemas[record_type].items()]\n",
    "            )\n",
    "\n",
    "def get_submission_schema(schema_path:Optional[str]=None) -> pa.Schema:\n",
    "    \"\"\"\n",
    "    Returns a boilerplate pyarrow schema for submission data. An optional schema path\n",
    "    can be provided to return a predefined schema.\n",
    "    Input:\n",
    "        - [Optional] path to JSON schema. Parent object must be either 'submission' or 'comment'\n",
    "        and child object must be a dictionary containing column name keys and pyarrow datatype values\n",
    "        Ex. 'submission' : {'submission_id' : 'pa.string()'}\n",
    "    Output:\n",
    "        - PyArrow Schema object\n",
    "    \"\"\"\n",
    "    if schema_path:\n",
    "        return load_schema(schema_path, 'submission')\n",
    "    return pa.schema([\n",
    "        (\"submission_id\", pa.string()),\n",
    "        (\"title\", pa.string()),\n",
    "        (\"selftext\", pa.string()),\n",
    "        (\"score\", pa.int64()),\n",
    "        (\"upvote_ratio\", pa.float64()),\n",
    "        (\"timestamp\", pa.timestamp(\"s\")),\n",
    "        (\"date\", pa.date32()),\n",
    "        (\"subreddit\", pa.string()),\n",
    "        (\"num_comments\", pa.int64()),\n",
    "    ])\n",
    "    \n",
    "def get_comment_schema(schema_path:Optional[str]=None) -> pa.Schema:\n",
    "    \"\"\"\n",
    "    Returns a boilerplate pyarrow schema for submission data. An optional schema path\n",
    "    can be provided to return a predefined schema.\n",
    "    Input:\n",
    "        - [Optional] path to JSON schema. Parent object must be either 'submission' or 'comment'\n",
    "        and child object must be a dictionary containing column name keys and pyarrow datatype values\n",
    "        Ex. 'submission' : {'submission_id' : 'pa.string()'}\n",
    "    Output:\n",
    "        - PyArrow Schema object\n",
    "    \"\"\"\n",
    "    if schema_path:\n",
    "        return load_schema(schema_path, 'comment')\n",
    "    return pa.schema([\n",
    "        (\"comment_id\", pa.string()),\n",
    "        (\"body\", pa.string()),\n",
    "        (\"score\", pa.int64()),\n",
    "        (\"timestamp\", pa.timestamp(\"s\")),\n",
    "        (\"date\", pa.date32()),\n",
    "        (\"subreddit\", pa.string()),\n",
    "        (\"parent_submission_id\", pa.string()),\n",
    "    ])\n",
    "\n",
    "def get_parquet_configs(\n",
    "    project_root:str, \n",
    "    subdir:str=\"data/processed\", \n",
    "    dataset_dirs:Union[Tuple[str],List[str]]=('submission-dataset','comment-dataset'),\n",
    "    **schema_kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Utility function to generate tuple of ParquetConfig namedtuples\n",
    "    Input:\n",
    "        - project root path\n",
    "        - data subdirectory path (default = data/processed)\n",
    "        - dataset directory names in order: 1) submission dataset, 2) comment dataset \n",
    "        (default = ('submission-dataset','comment-dataset'))\n",
    "    Output:\n",
    "        - tuple of namedtuples (sub_cfg, com_cfg) containing record_type, dataset_path, and\n",
    "        schema attribtues\n",
    "    \"\"\"\n",
    "    # Define the dataset directory paths for submission and content data\n",
    "    sub_path, com_path = tuple(\n",
    "        os.path.join(project_root, subdir, dataset_dir) \n",
    "        for dataset_dir in dataset_dirs\n",
    "    )\n",
    "    \n",
    "    # Get the submission and content data Arrow & Parquet schemas\n",
    "    sub_schema = get_submission_schema(**schema_kwargs)\n",
    "    com_schema = get_comment_schema(**schema_kwargs)\n",
    "\n",
    "    # Build the ParquetConfig files and return as tuple\n",
    "    return tuple(\n",
    "        ParquetConfig(record_type, path, schema) \n",
    "        for record_type, path, schema \n",
    "        in zip(\n",
    "            ('submission','comment'),\n",
    "            (sub_path, com_path),\n",
    "            (sub_schema, com_schema)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904ce7c",
   "metadata": {},
   "source": [
    "## Building the ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58413ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Streaming Prerequisites\n",
    "## Parse text files containing search queries and subreddit names\n",
    "search_queries, subreddits = get_search_pairs('..')\n",
    "\n",
    "# Only get a subset of search queries\n",
    "sample_queries = search_queries[0:1]\n",
    "sample_subreddits = subreddits[0:1]\n",
    "\n",
    "# Main ETL Pipeline\n",
    "PARQUET_CONFIG = get_parquet_configs('..', schema_path='../data/raw/schemas.json')\n",
    "\n",
    "## Initialize the streamer and loader classes, transformer will be initialized by the loader.\n",
    "streamer = DataStreamer(REDDIT)\n",
    "# transformer = DataTransformer()\n",
    "# loader = ParquetDataLoader(config=PARQUET_CONFIG, transformer=transformer)\n",
    "\n",
    "## Streamer will stream submission and search data, loader will store data to buffer, transformer\n",
    "## will preprocess the batched data, and then loader will finally write to disk.\n",
    "stream = streamer.stream(subreddits=sample_subreddits, queries=sample_queries, limit=50)\n",
    "# loader.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3ef9b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 781 ms, sys: 85.7 ms, total: 867 ms\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "etl_sample_stream = tuple(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b248be45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remaining': 948.0, 'reset_timestamp': 1751021999.491959, 'used': 52}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamer.rate_limiter.REDDIT.auth.limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "092b5d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51 total requests as of 2025-06-27 20:57:59.'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamer.rate_limiter.print_total_requests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b23c745b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1951"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(etl_sample_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39412b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 ms, sys: 6.79 ms, total: 28.2 ms\n",
      "Wall time: 25.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parse sample parquet files\n",
    "submissions_df = pd.read_parquet('../data/submission_data_0.parquet')\n",
    "comments_df = pd.read_parquet('../data/comment_data_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e20eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1d0xqjx</th>\n",
       "      <td>Most reliable used car, under $25k, less than ...</td>\n",
       "      <td>I’m thinking it’s likely a Toyota or Honda, bu...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.716716e+09</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1jx4l2a</th>\n",
       "      <td>Used SUV under $20k (need a reliable car ASAP)</td>\n",
       "      <td>_Reposting this because I didn’t get any comme...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.744417e+09</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18fr75g</th>\n",
       "      <td>What would be the cheap-to-run-and-maintain ca...</td>\n",
       "      <td>Hello, I am looking for a car with good fuel e...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.702289e+09</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1gjb97c</th>\n",
       "      <td>Just moved to Australia, looking for a used sm...</td>\n",
       "      <td>We have two small kids, but have access to a 4...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.730717e+09</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1dnb05z</th>\n",
       "      <td>Reliable used car for $10-15k</td>\n",
       "      <td>I’m in the market for a used car with a budget...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.719228e+09</td>\n",
       "      <td>r/CarsAustralia</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "1d0xqjx  Most reliable used car, under $25k, less than ...   \n",
       "1jx4l2a     Used SUV under $20k (need a reliable car ASAP)   \n",
       "18fr75g  What would be the cheap-to-run-and-maintain ca...   \n",
       "1gjb97c  Just moved to Australia, looking for a used sm...   \n",
       "1dnb05z                      Reliable used car for $10-15k   \n",
       "\n",
       "                                                  selftext  score  \\\n",
       "1d0xqjx  I’m thinking it’s likely a Toyota or Honda, bu...     29   \n",
       "1jx4l2a  _Reposting this because I didn’t get any comme...      2   \n",
       "18fr75g  Hello, I am looking for a car with good fuel e...     29   \n",
       "1gjb97c  We have two small kids, but have access to a 4...      1   \n",
       "1dnb05z  I’m in the market for a used car with a budget...      5   \n",
       "\n",
       "         upvote_ratio     timestamp        subreddit  num_comments  \n",
       "1d0xqjx          0.94  1.716716e+09  r/CarsAustralia           120  \n",
       "1jx4l2a          1.00  1.744417e+09  r/CarsAustralia            20  \n",
       "18fr75g          0.63  1.702289e+09  r/CarsAustralia            74  \n",
       "1gjb97c          0.57  1.730717e+09  r/CarsAustralia             9  \n",
       "1dnb05z          1.00  1.719228e+09  r/CarsAustralia            11  "
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c5949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l5q3ib3    Mazda 3, Corolla, i30 would be my pick. I had ...\n",
       "l5q6wxs                             Almost anything Japanese\n",
       "l5q37wv                                        Camry/corolla\n",
       "l5q74n2    Toyota Camry - boring as hell but super reliab...\n",
       "l5q6dmd                                    Honda accord euro\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dba01",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42fa6a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ff7ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
